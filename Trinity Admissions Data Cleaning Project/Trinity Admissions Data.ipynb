{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6d0cd88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>train-test</th>\n",
       "      <th>Entry Term (Application)</th>\n",
       "      <th>Admit Type</th>\n",
       "      <th>Permanent Postal</th>\n",
       "      <th>Permanent Country</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Ethnicity</th>\n",
       "      <th>Race</th>\n",
       "      <th>Religion</th>\n",
       "      <th>...</th>\n",
       "      <th>SAT Concordance Score (of SAT R)</th>\n",
       "      <th>ACT Concordance Score (of SAT R)</th>\n",
       "      <th>ACT Concordance Score (of SAT)</th>\n",
       "      <th>Test Optional</th>\n",
       "      <th>SAT I Critical Reading</th>\n",
       "      <th>SAT I Math</th>\n",
       "      <th>SAT I Writing</th>\n",
       "      <th>SAT R Evidence-Based Reading and Writing Section</th>\n",
       "      <th>SAT R Math Section</th>\n",
       "      <th>Decision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>Fall 2017</td>\n",
       "      <td>FY</td>\n",
       "      <td>87507-7944</td>\n",
       "      <td>United States</td>\n",
       "      <td>F</td>\n",
       "      <td>Non Hispanic/Latino</td>\n",
       "      <td>White</td>\n",
       "      <td>Roman Catholic</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>train</td>\n",
       "      <td>Fall 2019</td>\n",
       "      <td>FY</td>\n",
       "      <td>75082-2652</td>\n",
       "      <td>United States</td>\n",
       "      <td>M</td>\n",
       "      <td>Non Hispanic/Latino</td>\n",
       "      <td>White</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1320.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>660.0</td>\n",
       "      <td>720.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>train</td>\n",
       "      <td>Fall 2020</td>\n",
       "      <td>FY</td>\n",
       "      <td>77055-6522</td>\n",
       "      <td>United States</td>\n",
       "      <td>F</td>\n",
       "      <td>Hispanic/Latino</td>\n",
       "      <td>White</td>\n",
       "      <td>Christian</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>train</td>\n",
       "      <td>Fall 2017</td>\n",
       "      <td>FY</td>\n",
       "      <td>98607-8571</td>\n",
       "      <td>United States</td>\n",
       "      <td>F</td>\n",
       "      <td>Non Hispanic/Latino</td>\n",
       "      <td>Asian</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>750.0</td>\n",
       "      <td>770.0</td>\n",
       "      <td>700.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>train</td>\n",
       "      <td>Fall 2019</td>\n",
       "      <td>FY</td>\n",
       "      <td>78681-3451</td>\n",
       "      <td>United States</td>\n",
       "      <td>M</td>\n",
       "      <td>Non Hispanic/Latino</td>\n",
       "      <td>White</td>\n",
       "      <td>Christian</td>\n",
       "      <td>...</td>\n",
       "      <td>1290.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>650.0</td>\n",
       "      <td>700.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15138</th>\n",
       "      <td>15139</td>\n",
       "      <td>test</td>\n",
       "      <td>Fall 2020</td>\n",
       "      <td>FY</td>\n",
       "      <td>91006-1737</td>\n",
       "      <td>United States</td>\n",
       "      <td>F</td>\n",
       "      <td>Non Hispanic/Latino</td>\n",
       "      <td>Asian</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1270.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15139</th>\n",
       "      <td>15140</td>\n",
       "      <td>test</td>\n",
       "      <td>Fall 2019</td>\n",
       "      <td>FY</td>\n",
       "      <td>77494-5298</td>\n",
       "      <td>United States</td>\n",
       "      <td>F</td>\n",
       "      <td>Non Hispanic/Latino</td>\n",
       "      <td>Asian</td>\n",
       "      <td>Hindu</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15140</th>\n",
       "      <td>15141</td>\n",
       "      <td>test</td>\n",
       "      <td>Fall 2020</td>\n",
       "      <td>FY</td>\n",
       "      <td>55443-1016</td>\n",
       "      <td>United States</td>\n",
       "      <td>M</td>\n",
       "      <td>Non Hispanic/Latino</td>\n",
       "      <td>Black or African American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15141</th>\n",
       "      <td>15142</td>\n",
       "      <td>test</td>\n",
       "      <td>Fall 2018</td>\n",
       "      <td>FY</td>\n",
       "      <td>75024-2138</td>\n",
       "      <td>United States</td>\n",
       "      <td>F</td>\n",
       "      <td>Non Hispanic/Latino</td>\n",
       "      <td>White</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1470.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>770.0</td>\n",
       "      <td>740.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15142</th>\n",
       "      <td>15143</td>\n",
       "      <td>test</td>\n",
       "      <td>Fall 2019</td>\n",
       "      <td>FY</td>\n",
       "      <td>78624-6081</td>\n",
       "      <td>United States</td>\n",
       "      <td>F</td>\n",
       "      <td>Non Hispanic/Latino</td>\n",
       "      <td>White</td>\n",
       "      <td>Roman Catholic</td>\n",
       "      <td>...</td>\n",
       "      <td>1220.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>650.0</td>\n",
       "      <td>640.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15143 rows × 69 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID train-test Entry Term (Application) Admit Type Permanent Postal  \\\n",
       "0          1      train                Fall 2017         FY       87507-7944   \n",
       "1          2      train                Fall 2019         FY       75082-2652   \n",
       "2          3      train                Fall 2020         FY       77055-6522   \n",
       "3          4      train                Fall 2017         FY       98607-8571   \n",
       "4          5      train                Fall 2019         FY       78681-3451   \n",
       "...      ...        ...                      ...        ...              ...   \n",
       "15138  15139       test                Fall 2020         FY       91006-1737   \n",
       "15139  15140       test                Fall 2019         FY       77494-5298   \n",
       "15140  15141       test                Fall 2020         FY       55443-1016   \n",
       "15141  15142       test                Fall 2018         FY       75024-2138   \n",
       "15142  15143       test                Fall 2019         FY       78624-6081   \n",
       "\n",
       "      Permanent Country Sex            Ethnicity                       Race  \\\n",
       "0         United States   F  Non Hispanic/Latino                      White   \n",
       "1         United States   M  Non Hispanic/Latino                      White   \n",
       "2         United States   F      Hispanic/Latino                      White   \n",
       "3         United States   F  Non Hispanic/Latino                      Asian   \n",
       "4         United States   M  Non Hispanic/Latino                      White   \n",
       "...                 ...  ..                  ...                        ...   \n",
       "15138     United States   F  Non Hispanic/Latino                      Asian   \n",
       "15139     United States   F  Non Hispanic/Latino                      Asian   \n",
       "15140     United States   M  Non Hispanic/Latino  Black or African American   \n",
       "15141     United States   F  Non Hispanic/Latino                      White   \n",
       "15142     United States   F  Non Hispanic/Latino                      White   \n",
       "\n",
       "             Religion  ... SAT Concordance Score (of SAT R)  \\\n",
       "0      Roman Catholic  ...                              NaN   \n",
       "1                 NaN  ...                           1320.0   \n",
       "2           Christian  ...                              NaN   \n",
       "3                 NaN  ...                              NaN   \n",
       "4           Christian  ...                           1290.0   \n",
       "...               ...  ...                              ...   \n",
       "15138             NaN  ...                           1270.0   \n",
       "15139           Hindu  ...                              NaN   \n",
       "15140             NaN  ...                              NaN   \n",
       "15141             NaN  ...                           1470.0   \n",
       "15142  Roman Catholic  ...                           1220.0   \n",
       "\n",
       "      ACT Concordance Score (of SAT R) ACT Concordance Score (of SAT)  \\\n",
       "0                                  NaN                            NaN   \n",
       "1                                 30.0                            NaN   \n",
       "2                                  NaN                            NaN   \n",
       "3                                  NaN                            NaN   \n",
       "4                                 29.0                            NaN   \n",
       "...                                ...                            ...   \n",
       "15138                             29.0                            NaN   \n",
       "15139                              NaN                            NaN   \n",
       "15140                              NaN                            NaN   \n",
       "15141                             33.0                            NaN   \n",
       "15142                             27.0                            NaN   \n",
       "\n",
       "      Test Optional SAT I Critical Reading SAT I Math SAT I Writing  \\\n",
       "0               NaN                    NaN        NaN           NaN   \n",
       "1               NaN                    NaN        NaN           NaN   \n",
       "2               NaN                    NaN        NaN           NaN   \n",
       "3               NaN                  750.0      770.0         700.0   \n",
       "4               NaN                    NaN        NaN           NaN   \n",
       "...             ...                    ...        ...           ...   \n",
       "15138           NaN                    NaN        NaN           NaN   \n",
       "15139           NaN                    NaN        NaN           NaN   \n",
       "15140           NaN                    NaN        NaN           NaN   \n",
       "15141           NaN                    NaN        NaN           NaN   \n",
       "15142           NaN                    NaN        NaN           NaN   \n",
       "\n",
       "      SAT R Evidence-Based Reading and Writing Section SAT R Math Section  \\\n",
       "0                                                  NaN                NaN   \n",
       "1                                                660.0              720.0   \n",
       "2                                                  NaN                NaN   \n",
       "3                                                  NaN                NaN   \n",
       "4                                                650.0              700.0   \n",
       "...                                                ...                ...   \n",
       "15138                                              NaN                NaN   \n",
       "15139                                              NaN                NaN   \n",
       "15140                                              NaN                NaN   \n",
       "15141                                            770.0              740.0   \n",
       "15142                                            650.0              640.0   \n",
       "\n",
       "      Decision  \n",
       "0            1  \n",
       "1            0  \n",
       "2            1  \n",
       "3            0  \n",
       "4            0  \n",
       "...        ...  \n",
       "15138        0  \n",
       "15139        0  \n",
       "15140        0  \n",
       "15141        0  \n",
       "15142        1  \n",
       "\n",
       "[15143 rows x 69 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#Read in TU.csv\n",
    "TU = pd.read_csv(\"TU.csv\")\n",
    "TU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "959e0e36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'train-test', 'Entry Term (Application)', 'Admit Type',\n",
       "       'Permanent Postal', 'Permanent Country', 'Sex', 'Ethnicity', 'Race',\n",
       "       'Religion', 'First_Source Origin First Source Date', 'Inquiry Date',\n",
       "       'Submitted', 'Application Source', 'Decision Plan',\n",
       "       'Staff Assigned Name', 'Legacy', 'Athlete', 'Sport 1 Sport',\n",
       "       'Sport 1 Rating', 'Sport 2 Sport', 'Sport 2 Rating', 'Sport 3 Sport',\n",
       "       'Sport 3 Rating', 'Academic Interest 1', 'Academic Interest 2',\n",
       "       'First_Source Origin First Source Summary', 'Total Event Participation',\n",
       "       'Count of Campus Visits', 'School #1 Organization Category',\n",
       "       'School 1 Code', 'School 1 Class Rank (Numeric)',\n",
       "       'School 1 Class Size (Numeric)', 'School 1 GPA', 'School 1 GPA Scale',\n",
       "       'School 1 GPA Recalculated', 'School 2 Class Rank (Numeric)',\n",
       "       'School 2 Class Size (Numeric)', 'School 2 GPA', 'School 2 GPA Scale',\n",
       "       'School 2 GPA Recalculated', 'School 3 Class Rank (Numeric)',\n",
       "       'School 3 Class Size (Numeric)', 'School 3 GPA', 'School 3 GPA Scale',\n",
       "       'School 3 GPA Recalculated', 'ACT Composite', 'ACT English',\n",
       "       'ACT Reading', 'ACT Math', 'ACT Science Reasoning', 'ACT Writing',\n",
       "       'SAT I CR + M',\n",
       "       'SAT R Evidence-Based Reading and Writing Section + Math Section',\n",
       "       'Permanent Geomarket', 'Citizenship Status', 'Academic Index',\n",
       "       'Intend to Apply for Financial Aid?', 'Merit Award',\n",
       "       'SAT Concordance Score (of SAT R)', 'ACT Concordance Score (of SAT R)',\n",
       "       'ACT Concordance Score (of SAT)', 'Test Optional',\n",
       "       'SAT I Critical Reading', 'SAT I Math', 'SAT I Writing',\n",
       "       'SAT R Evidence-Based Reading and Writing Section',\n",
       "       'SAT R Math Section', 'Decision'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TU.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38149e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "['FY']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>train-test</th>\n",
       "      <th>Entry Term (Application)</th>\n",
       "      <th>Permanent Postal</th>\n",
       "      <th>Permanent Country</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Ethnicity</th>\n",
       "      <th>Race</th>\n",
       "      <th>Religion</th>\n",
       "      <th>First_Source Origin First Source Date</th>\n",
       "      <th>...</th>\n",
       "      <th>SAT Concordance Score (of SAT R)</th>\n",
       "      <th>ACT Concordance Score (of SAT R)</th>\n",
       "      <th>ACT Concordance Score (of SAT)</th>\n",
       "      <th>Test Optional</th>\n",
       "      <th>SAT I Critical Reading</th>\n",
       "      <th>SAT I Math</th>\n",
       "      <th>SAT I Writing</th>\n",
       "      <th>SAT R Evidence-Based Reading and Writing Section</th>\n",
       "      <th>SAT R Math Section</th>\n",
       "      <th>Decision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>Fall 2017</td>\n",
       "      <td>87507-7944</td>\n",
       "      <td>United States</td>\n",
       "      <td>F</td>\n",
       "      <td>Non Hispanic/Latino</td>\n",
       "      <td>White</td>\n",
       "      <td>Roman Catholic</td>\n",
       "      <td>11/18/2016 5:40</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>train</td>\n",
       "      <td>Fall 2019</td>\n",
       "      <td>75082-2652</td>\n",
       "      <td>United States</td>\n",
       "      <td>M</td>\n",
       "      <td>Non Hispanic/Latino</td>\n",
       "      <td>White</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1/30/2017 17:24</td>\n",
       "      <td>...</td>\n",
       "      <td>1320.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>660.0</td>\n",
       "      <td>720.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>train</td>\n",
       "      <td>Fall 2020</td>\n",
       "      <td>77055-6522</td>\n",
       "      <td>United States</td>\n",
       "      <td>F</td>\n",
       "      <td>Hispanic/Latino</td>\n",
       "      <td>White</td>\n",
       "      <td>Christian</td>\n",
       "      <td>1/31/2019 12:35</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>train</td>\n",
       "      <td>Fall 2017</td>\n",
       "      <td>98607-8571</td>\n",
       "      <td>United States</td>\n",
       "      <td>F</td>\n",
       "      <td>Non Hispanic/Latino</td>\n",
       "      <td>Asian</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6/15/2015</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>750.0</td>\n",
       "      <td>770.0</td>\n",
       "      <td>700.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>train</td>\n",
       "      <td>Fall 2019</td>\n",
       "      <td>78681-3451</td>\n",
       "      <td>United States</td>\n",
       "      <td>M</td>\n",
       "      <td>Non Hispanic/Latino</td>\n",
       "      <td>White</td>\n",
       "      <td>Christian</td>\n",
       "      <td>2/19/2018 11:11</td>\n",
       "      <td>...</td>\n",
       "      <td>1290.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>650.0</td>\n",
       "      <td>700.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15138</th>\n",
       "      <td>15139</td>\n",
       "      <td>test</td>\n",
       "      <td>Fall 2020</td>\n",
       "      <td>91006-1737</td>\n",
       "      <td>United States</td>\n",
       "      <td>F</td>\n",
       "      <td>Non Hispanic/Latino</td>\n",
       "      <td>Asian</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2/19/2018 11:11</td>\n",
       "      <td>...</td>\n",
       "      <td>1270.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15139</th>\n",
       "      <td>15140</td>\n",
       "      <td>test</td>\n",
       "      <td>Fall 2019</td>\n",
       "      <td>77494-5298</td>\n",
       "      <td>United States</td>\n",
       "      <td>F</td>\n",
       "      <td>Non Hispanic/Latino</td>\n",
       "      <td>Asian</td>\n",
       "      <td>Hindu</td>\n",
       "      <td>2/16/2018 16:31</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15140</th>\n",
       "      <td>15141</td>\n",
       "      <td>test</td>\n",
       "      <td>Fall 2020</td>\n",
       "      <td>55443-1016</td>\n",
       "      <td>United States</td>\n",
       "      <td>M</td>\n",
       "      <td>Non Hispanic/Latino</td>\n",
       "      <td>Black or African American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6/25/2019 12:50</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15141</th>\n",
       "      <td>15142</td>\n",
       "      <td>test</td>\n",
       "      <td>Fall 2018</td>\n",
       "      <td>75024-2138</td>\n",
       "      <td>United States</td>\n",
       "      <td>F</td>\n",
       "      <td>Non Hispanic/Latino</td>\n",
       "      <td>White</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2/3/2016</td>\n",
       "      <td>...</td>\n",
       "      <td>1470.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>770.0</td>\n",
       "      <td>740.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15142</th>\n",
       "      <td>15143</td>\n",
       "      <td>test</td>\n",
       "      <td>Fall 2019</td>\n",
       "      <td>78624-6081</td>\n",
       "      <td>United States</td>\n",
       "      <td>F</td>\n",
       "      <td>Non Hispanic/Latino</td>\n",
       "      <td>White</td>\n",
       "      <td>Roman Catholic</td>\n",
       "      <td>1/30/2017 17:24</td>\n",
       "      <td>...</td>\n",
       "      <td>1220.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>650.0</td>\n",
       "      <td>640.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15143 rows × 68 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID train-test Entry Term (Application) Permanent Postal  \\\n",
       "0          1      train                Fall 2017       87507-7944   \n",
       "1          2      train                Fall 2019       75082-2652   \n",
       "2          3      train                Fall 2020       77055-6522   \n",
       "3          4      train                Fall 2017       98607-8571   \n",
       "4          5      train                Fall 2019       78681-3451   \n",
       "...      ...        ...                      ...              ...   \n",
       "15138  15139       test                Fall 2020       91006-1737   \n",
       "15139  15140       test                Fall 2019       77494-5298   \n",
       "15140  15141       test                Fall 2020       55443-1016   \n",
       "15141  15142       test                Fall 2018       75024-2138   \n",
       "15142  15143       test                Fall 2019       78624-6081   \n",
       "\n",
       "      Permanent Country Sex            Ethnicity                       Race  \\\n",
       "0         United States   F  Non Hispanic/Latino                      White   \n",
       "1         United States   M  Non Hispanic/Latino                      White   \n",
       "2         United States   F      Hispanic/Latino                      White   \n",
       "3         United States   F  Non Hispanic/Latino                      Asian   \n",
       "4         United States   M  Non Hispanic/Latino                      White   \n",
       "...                 ...  ..                  ...                        ...   \n",
       "15138     United States   F  Non Hispanic/Latino                      Asian   \n",
       "15139     United States   F  Non Hispanic/Latino                      Asian   \n",
       "15140     United States   M  Non Hispanic/Latino  Black or African American   \n",
       "15141     United States   F  Non Hispanic/Latino                      White   \n",
       "15142     United States   F  Non Hispanic/Latino                      White   \n",
       "\n",
       "             Religion First_Source Origin First Source Date  ...  \\\n",
       "0      Roman Catholic                       11/18/2016 5:40  ...   \n",
       "1                 NaN                       1/30/2017 17:24  ...   \n",
       "2           Christian                       1/31/2019 12:35  ...   \n",
       "3                 NaN                             6/15/2015  ...   \n",
       "4           Christian                       2/19/2018 11:11  ...   \n",
       "...               ...                                   ...  ...   \n",
       "15138             NaN                       2/19/2018 11:11  ...   \n",
       "15139           Hindu                       2/16/2018 16:31  ...   \n",
       "15140             NaN                       6/25/2019 12:50  ...   \n",
       "15141             NaN                              2/3/2016  ...   \n",
       "15142  Roman Catholic                       1/30/2017 17:24  ...   \n",
       "\n",
       "      SAT Concordance Score (of SAT R) ACT Concordance Score (of SAT R)  \\\n",
       "0                                  NaN                              NaN   \n",
       "1                               1320.0                             30.0   \n",
       "2                                  NaN                              NaN   \n",
       "3                                  NaN                              NaN   \n",
       "4                               1290.0                             29.0   \n",
       "...                                ...                              ...   \n",
       "15138                           1270.0                             29.0   \n",
       "15139                              NaN                              NaN   \n",
       "15140                              NaN                              NaN   \n",
       "15141                           1470.0                             33.0   \n",
       "15142                           1220.0                             27.0   \n",
       "\n",
       "      ACT Concordance Score (of SAT) Test Optional SAT I Critical Reading  \\\n",
       "0                                NaN           NaN                    NaN   \n",
       "1                                NaN           NaN                    NaN   \n",
       "2                                NaN           NaN                    NaN   \n",
       "3                                NaN           NaN                  750.0   \n",
       "4                                NaN           NaN                    NaN   \n",
       "...                              ...           ...                    ...   \n",
       "15138                            NaN           NaN                    NaN   \n",
       "15139                            NaN           NaN                    NaN   \n",
       "15140                            NaN           NaN                    NaN   \n",
       "15141                            NaN           NaN                    NaN   \n",
       "15142                            NaN           NaN                    NaN   \n",
       "\n",
       "      SAT I Math SAT I Writing  \\\n",
       "0            NaN           NaN   \n",
       "1            NaN           NaN   \n",
       "2            NaN           NaN   \n",
       "3          770.0         700.0   \n",
       "4            NaN           NaN   \n",
       "...          ...           ...   \n",
       "15138        NaN           NaN   \n",
       "15139        NaN           NaN   \n",
       "15140        NaN           NaN   \n",
       "15141        NaN           NaN   \n",
       "15142        NaN           NaN   \n",
       "\n",
       "      SAT R Evidence-Based Reading and Writing Section SAT R Math Section  \\\n",
       "0                                                  NaN                NaN   \n",
       "1                                                660.0              720.0   \n",
       "2                                                  NaN                NaN   \n",
       "3                                                  NaN                NaN   \n",
       "4                                                650.0              700.0   \n",
       "...                                                ...                ...   \n",
       "15138                                              NaN                NaN   \n",
       "15139                                              NaN                NaN   \n",
       "15140                                              NaN                NaN   \n",
       "15141                                            770.0              740.0   \n",
       "15142                                            650.0              640.0   \n",
       "\n",
       "      Decision  \n",
       "0            1  \n",
       "1            0  \n",
       "2            1  \n",
       "3            0  \n",
       "4            0  \n",
       "...        ...  \n",
       "15138        0  \n",
       "15139        0  \n",
       "15140        0  \n",
       "15141        0  \n",
       "15142        1  \n",
       "\n",
       "[15143 rows x 68 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3 Admit Type (removing variables)\n",
    "print(TU['Admit Type'].isna().sum())\n",
    "#No NA.\n",
    "print(TU['Admit Type'].unique())\n",
    "#No irregular categories.\n",
    "\n",
    "#Since the data set only has first years (i.e.,only one category), \n",
    "# Admit.Type should be removed in the modeling stage.\n",
    "TU=TU.drop('Admit Type',axis='columns')\n",
    "TU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fefa62df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162\n",
      "['87507-7944' '75082-2652' '77055-6522' ... '77494-5298' '55443-1016'\n",
      " '78624-6081']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>train-test</th>\n",
       "      <th>Entry Term (Application)</th>\n",
       "      <th>Permanent Country</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Ethnicity</th>\n",
       "      <th>Race</th>\n",
       "      <th>Religion</th>\n",
       "      <th>First_Source Origin First Source Date</th>\n",
       "      <th>Inquiry Date</th>\n",
       "      <th>...</th>\n",
       "      <th>SAT Concordance Score (of SAT R)</th>\n",
       "      <th>ACT Concordance Score (of SAT R)</th>\n",
       "      <th>ACT Concordance Score (of SAT)</th>\n",
       "      <th>Test Optional</th>\n",
       "      <th>SAT I Critical Reading</th>\n",
       "      <th>SAT I Math</th>\n",
       "      <th>SAT I Writing</th>\n",
       "      <th>SAT R Evidence-Based Reading and Writing Section</th>\n",
       "      <th>SAT R Math Section</th>\n",
       "      <th>Decision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>Fall 2017</td>\n",
       "      <td>United States</td>\n",
       "      <td>F</td>\n",
       "      <td>Non Hispanic/Latino</td>\n",
       "      <td>White</td>\n",
       "      <td>Roman Catholic</td>\n",
       "      <td>11/18/2016 5:40</td>\n",
       "      <td>2/13/2017 19:25</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>train</td>\n",
       "      <td>Fall 2019</td>\n",
       "      <td>United States</td>\n",
       "      <td>M</td>\n",
       "      <td>Non Hispanic/Latino</td>\n",
       "      <td>White</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1/30/2017 17:24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1320.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>660.0</td>\n",
       "      <td>720.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>train</td>\n",
       "      <td>Fall 2020</td>\n",
       "      <td>United States</td>\n",
       "      <td>F</td>\n",
       "      <td>Hispanic/Latino</td>\n",
       "      <td>White</td>\n",
       "      <td>Christian</td>\n",
       "      <td>1/31/2019 12:35</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>train</td>\n",
       "      <td>Fall 2017</td>\n",
       "      <td>United States</td>\n",
       "      <td>F</td>\n",
       "      <td>Non Hispanic/Latino</td>\n",
       "      <td>Asian</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6/15/2015</td>\n",
       "      <td>10/18/2016 15:49</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>750.0</td>\n",
       "      <td>770.0</td>\n",
       "      <td>700.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>train</td>\n",
       "      <td>Fall 2019</td>\n",
       "      <td>United States</td>\n",
       "      <td>M</td>\n",
       "      <td>Non Hispanic/Latino</td>\n",
       "      <td>White</td>\n",
       "      <td>Christian</td>\n",
       "      <td>2/19/2018 11:11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1290.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>650.0</td>\n",
       "      <td>700.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15138</th>\n",
       "      <td>15139</td>\n",
       "      <td>test</td>\n",
       "      <td>Fall 2020</td>\n",
       "      <td>United States</td>\n",
       "      <td>F</td>\n",
       "      <td>Non Hispanic/Latino</td>\n",
       "      <td>Asian</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2/19/2018 11:11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1270.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15139</th>\n",
       "      <td>15140</td>\n",
       "      <td>test</td>\n",
       "      <td>Fall 2019</td>\n",
       "      <td>United States</td>\n",
       "      <td>F</td>\n",
       "      <td>Non Hispanic/Latino</td>\n",
       "      <td>Asian</td>\n",
       "      <td>Hindu</td>\n",
       "      <td>2/16/2018 16:31</td>\n",
       "      <td>8/28/2018 9:26</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15140</th>\n",
       "      <td>15141</td>\n",
       "      <td>test</td>\n",
       "      <td>Fall 2020</td>\n",
       "      <td>United States</td>\n",
       "      <td>M</td>\n",
       "      <td>Non Hispanic/Latino</td>\n",
       "      <td>Black or African American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6/25/2019 12:50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15141</th>\n",
       "      <td>15142</td>\n",
       "      <td>test</td>\n",
       "      <td>Fall 2018</td>\n",
       "      <td>United States</td>\n",
       "      <td>F</td>\n",
       "      <td>Non Hispanic/Latino</td>\n",
       "      <td>White</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2/3/2016</td>\n",
       "      <td>9/10/2017 10:24</td>\n",
       "      <td>...</td>\n",
       "      <td>1470.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>770.0</td>\n",
       "      <td>740.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15142</th>\n",
       "      <td>15143</td>\n",
       "      <td>test</td>\n",
       "      <td>Fall 2019</td>\n",
       "      <td>United States</td>\n",
       "      <td>F</td>\n",
       "      <td>Non Hispanic/Latino</td>\n",
       "      <td>White</td>\n",
       "      <td>Roman Catholic</td>\n",
       "      <td>1/30/2017 17:24</td>\n",
       "      <td>2/19/2018 11:19</td>\n",
       "      <td>...</td>\n",
       "      <td>1220.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>650.0</td>\n",
       "      <td>640.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15143 rows × 67 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID train-test Entry Term (Application) Permanent Country Sex  \\\n",
       "0          1      train                Fall 2017     United States   F   \n",
       "1          2      train                Fall 2019     United States   M   \n",
       "2          3      train                Fall 2020     United States   F   \n",
       "3          4      train                Fall 2017     United States   F   \n",
       "4          5      train                Fall 2019     United States   M   \n",
       "...      ...        ...                      ...               ...  ..   \n",
       "15138  15139       test                Fall 2020     United States   F   \n",
       "15139  15140       test                Fall 2019     United States   F   \n",
       "15140  15141       test                Fall 2020     United States   M   \n",
       "15141  15142       test                Fall 2018     United States   F   \n",
       "15142  15143       test                Fall 2019     United States   F   \n",
       "\n",
       "                 Ethnicity                       Race        Religion  \\\n",
       "0      Non Hispanic/Latino                      White  Roman Catholic   \n",
       "1      Non Hispanic/Latino                      White             NaN   \n",
       "2          Hispanic/Latino                      White       Christian   \n",
       "3      Non Hispanic/Latino                      Asian             NaN   \n",
       "4      Non Hispanic/Latino                      White       Christian   \n",
       "...                    ...                        ...             ...   \n",
       "15138  Non Hispanic/Latino                      Asian             NaN   \n",
       "15139  Non Hispanic/Latino                      Asian           Hindu   \n",
       "15140  Non Hispanic/Latino  Black or African American             NaN   \n",
       "15141  Non Hispanic/Latino                      White             NaN   \n",
       "15142  Non Hispanic/Latino                      White  Roman Catholic   \n",
       "\n",
       "      First_Source Origin First Source Date      Inquiry Date  ...  \\\n",
       "0                           11/18/2016 5:40   2/13/2017 19:25  ...   \n",
       "1                           1/30/2017 17:24               NaN  ...   \n",
       "2                           1/31/2019 12:35               NaN  ...   \n",
       "3                                 6/15/2015  10/18/2016 15:49  ...   \n",
       "4                           2/19/2018 11:11               NaN  ...   \n",
       "...                                     ...               ...  ...   \n",
       "15138                       2/19/2018 11:11               NaN  ...   \n",
       "15139                       2/16/2018 16:31    8/28/2018 9:26  ...   \n",
       "15140                       6/25/2019 12:50               NaN  ...   \n",
       "15141                              2/3/2016   9/10/2017 10:24  ...   \n",
       "15142                       1/30/2017 17:24   2/19/2018 11:19  ...   \n",
       "\n",
       "      SAT Concordance Score (of SAT R) ACT Concordance Score (of SAT R)  \\\n",
       "0                                  NaN                              NaN   \n",
       "1                               1320.0                             30.0   \n",
       "2                                  NaN                              NaN   \n",
       "3                                  NaN                              NaN   \n",
       "4                               1290.0                             29.0   \n",
       "...                                ...                              ...   \n",
       "15138                           1270.0                             29.0   \n",
       "15139                              NaN                              NaN   \n",
       "15140                              NaN                              NaN   \n",
       "15141                           1470.0                             33.0   \n",
       "15142                           1220.0                             27.0   \n",
       "\n",
       "      ACT Concordance Score (of SAT) Test Optional SAT I Critical Reading  \\\n",
       "0                                NaN           NaN                    NaN   \n",
       "1                                NaN           NaN                    NaN   \n",
       "2                                NaN           NaN                    NaN   \n",
       "3                                NaN           NaN                  750.0   \n",
       "4                                NaN           NaN                    NaN   \n",
       "...                              ...           ...                    ...   \n",
       "15138                            NaN           NaN                    NaN   \n",
       "15139                            NaN           NaN                    NaN   \n",
       "15140                            NaN           NaN                    NaN   \n",
       "15141                            NaN           NaN                    NaN   \n",
       "15142                            NaN           NaN                    NaN   \n",
       "\n",
       "      SAT I Math SAT I Writing  \\\n",
       "0            NaN           NaN   \n",
       "1            NaN           NaN   \n",
       "2            NaN           NaN   \n",
       "3          770.0         700.0   \n",
       "4            NaN           NaN   \n",
       "...          ...           ...   \n",
       "15138        NaN           NaN   \n",
       "15139        NaN           NaN   \n",
       "15140        NaN           NaN   \n",
       "15141        NaN           NaN   \n",
       "15142        NaN           NaN   \n",
       "\n",
       "      SAT R Evidence-Based Reading and Writing Section SAT R Math Section  \\\n",
       "0                                                  NaN                NaN   \n",
       "1                                                660.0              720.0   \n",
       "2                                                  NaN                NaN   \n",
       "3                                                  NaN                NaN   \n",
       "4                                                650.0              700.0   \n",
       "...                                                ...                ...   \n",
       "15138                                              NaN                NaN   \n",
       "15139                                              NaN                NaN   \n",
       "15140                                              NaN                NaN   \n",
       "15141                                            770.0              740.0   \n",
       "15142                                            650.0              640.0   \n",
       "\n",
       "      Decision  \n",
       "0            1  \n",
       "1            0  \n",
       "2            1  \n",
       "3            0  \n",
       "4            0  \n",
       "...        ...  \n",
       "15138        0  \n",
       "15139        0  \n",
       "15140        0  \n",
       "15141        0  \n",
       "15142        1  \n",
       "\n",
       "[15143 rows x 67 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Permanent Postal\n",
    "print(TU['Permanent Postal'].isna().sum())\n",
    "print(TU['Permanent Postal'].unique())\n",
    "TU = TU.drop('Permanent Postal', axis='columns')\n",
    "TU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b454111b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "['United States' 'Jamaica' 'Costa Rica' 'China' 'Vietnam' 'Nicaragua'\n",
      " 'Spain' 'India' 'Luxembourg' 'Nepal' 'Ecuador' 'Honduras' 'Cameroon'\n",
      " 'Mexico' 'Canada' 'Singapore' 'Bangladesh' 'Pakistan'\n",
      " 'United Arab Emirates' 'Uzbekistan' 'France' 'Thailand' 'Venezuela'\n",
      " 'Hong Kong S.A.R.' 'Switzerland' 'Tanzania' 'Brazil' 'El Salvador'\n",
      " 'Indonesia' 'Mozambique' 'Turkey' 'Czech Republic' 'Taiwan' 'Japan'\n",
      " 'South Korea' 'Colombia' \"Cote D'Ivoire\" 'Jordan' 'Kazakhstan' 'Panama'\n",
      " 'Belgium' 'United Kingdom' 'Nigeria' 'Peru' 'Lebanon' 'Cayman Islands'\n",
      " 'Guatemala' 'Argentina' 'Bolivia' 'Italy' 'Poland' 'Trinidad and Tobago'\n",
      " 'New Zealand' 'Ethiopia' 'Kenya' 'Montenegro' 'Germany' 'Saudi Arabia'\n",
      " 'Philippines' 'Greece' 'Ireland' 'Georgia' 'Belize' 'Netherlands'\n",
      " 'Palestine' 'Bosnia and Herzegovina' 'Cyprus' 'Norway' 'Russia'\n",
      " 'Barbados' 'Kuwait' 'Uruguay' 'Morocco' 'Ghana' 'The Bahamas'\n",
      " 'South Africa' 'Paraguay' 'Cambodia' 'Malaysia' 'Dominica' 'Iran'\n",
      " 'Lithuania' 'Iceland' 'Egypt' 'Australia' nan 'Ukraine' 'Romania'\n",
      " 'Uganda' 'Portugal' 'Zimbabwe' 'Oman' 'Mongolia' 'Chile' 'Albania'\n",
      " 'Dominican Republic']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'United States'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Column 5 Permanent Country\n",
    "print(TU['Permanent Country'].isna().sum())\n",
    "#1 missing value in this column \n",
    "print(TU['Permanent Country'].unique())\n",
    "# No duplicated countries are present\n",
    "TU[TU['Permanent Country'].isna()][\"Citizenship Status\"]\n",
    "TU.at[11147, 'Permanent Country'] = \"United States\"\n",
    "TU.iloc[11147]['Permanent Country']\n",
    "#TU - runs code to check if data is cleaned "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbee4192",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Column 6 Sex - no need for operations (very consistent formatting) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9979aca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "227\n",
      "['Non Hispanic/Latino' 'Hispanic/Latino' nan]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#Column 7 - Ethnicity (handling missing values)\n",
    "print(TU['Ethnicity'].isna().sum())\n",
    "# This column has 227 missing values \n",
    "print(TU['Ethnicity'].unique())\n",
    "# No irregular categories\n",
    "# Replace singular missing values with Not Specified\n",
    "TU['Ethnicity'].fillna(\"Not Specified\", inplace = True)\n",
    "print(TU['Ethnicity'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ed574ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "555\n",
      "['White' 'Asian' 'Black or African American' 'Asian, White' nan\n",
      " 'American Indian or Alaska Native, Black or African American, White'\n",
      " 'Black or African American, White'\n",
      " 'Asian, Black or African American, White'\n",
      " 'American Indian or Alaska Native'\n",
      " 'American Indian or Alaska Native, White'\n",
      " 'American Indian or Alaska Native, Asian, White'\n",
      " 'Asian, Black or African American' 'Native Hawaiian or Other Pacific'\n",
      " 'Asian, Native Hawaiian or Other Pacific'\n",
      " 'Native Hawaiian or Other Pacific, White'\n",
      " 'Asian, Native Hawaiian or Other Pacific, White'\n",
      " 'American Indian or Alaska Native, Black or African American'\n",
      " 'American Indian or Alaska Native, Asian'\n",
      " 'Black or African American, Native Hawaiian or Other Pacific'\n",
      " 'American Indian or Alaska Native, Native Hawaiian or Other Pacific'\n",
      " 'American Indian or Alaska Native, Asian, Black or African American, White'\n",
      " 'Asian, Black or African American, Native Hawaiian or Other Pacific, White'\n",
      " 'American Indian or Alaska Native, Asian, Black or African American, Native Hawaiian or Other Pacific, White'\n",
      " 'Black or African American, Native Hawaiian or Other Pacific, White']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "White                                                                                                          10316\n",
       "Asian                                                                                                           2483\n",
       "Black or African American                                                                                        760\n",
       "Not Specified                                                                                                    555\n",
       "Asian, White                                                                                                     458\n",
       "American Indian or Alaska Native, White                                                                          155\n",
       "Black or African American, White                                                                                 144\n",
       "American Indian or Alaska Native                                                                                 121\n",
       "Asian, Black or African American                                                                                  24\n",
       "Asian, Native Hawaiian or Other Pacific, White                                                                    21\n",
       "Asian, Native Hawaiian or Other Pacific                                                                           21\n",
       "Native Hawaiian or Other Pacific                                                                                  18\n",
       "Native Hawaiian or Other Pacific, White                                                                           16\n",
       "Asian, Black or African American, White                                                                           14\n",
       "American Indian or Alaska Native, Black or African American, White                                                12\n",
       "American Indian or Alaska Native, Black or African American                                                        8\n",
       "American Indian or Alaska Native, Asian, White                                                                     7\n",
       "American Indian or Alaska Native, Asian                                                                            3\n",
       "American Indian or Alaska Native, Asian, Black or African American, White                                          2\n",
       "Black or African American, Native Hawaiian or Other Pacific                                                        1\n",
       "American Indian or Alaska Native, Native Hawaiian or Other Pacific                                                 1\n",
       "Asian, Black or African American, Native Hawaiian or Other Pacific, White                                          1\n",
       "American Indian or Alaska Native, Asian, Black or African American, Native Hawaiian or Other Pacific, White        1\n",
       "Black or African American, Native Hawaiian or Other Pacific, White                                                 1\n",
       "Name: Race, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Column 8 - Race (similar process as column 7)\n",
    "\n",
    "# Handling missing values\n",
    "print(TU['Race'].isna().sum())\n",
    "#555 missing values in this column \n",
    "print(TU['Race'].unique())\n",
    "# No irregular categories\n",
    "# Fill nas as \"Not specified\"\n",
    "TU['Race'].fillna(\"Not Specified\", inplace = True)\n",
    "TU['Race'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc5a8d48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "White                                      10316\n",
       "Asian                                       2483\n",
       "Black or African American                    760\n",
       "Not Specified                                555\n",
       "Asian, White                                 458\n",
       "Others                                       272\n",
       "American Indian or Alaska Native, White      155\n",
       "Black or African American, White             144\n",
       "Name: Race, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Column 8 - augmenting existing data points\n",
    "RaceList = list(TU['Race'].value_counts()[:7].index)\n",
    "TU['Race'] = \\\n",
    "TU['Race'].apply(lambda x: 'Others' if x not in RaceList else x)\n",
    "TU['Race'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d53385a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5483\n",
      "['Roman Catholic' nan 'Christian' 'Presbyterian' 'Islam/Muslim' 'Jewish'\n",
      " 'Hindu' 'None' 'Baptist' 'Methodist' 'Jain' 'Anglican' 'Lutheran' 'Other'\n",
      " 'Assembly of God' 'Non-Denominational' 'Bible Churches'\n",
      " 'Christian Reformed' 'Unitarian' 'Eastern Orthodox' 'Episcopal'\n",
      " 'United Methodist' 'Church of Christ' 'Pentecostal'\n",
      " 'Lutheran-Missouri Synod' 'Protestant' 'Mormon-Latter Day Saints'\n",
      " 'Buddhism' 'Sikh' 'Church of God' 'Presbyterian Church of America'\n",
      " 'Evangelical' 'Southern Baptist' 'Society of Friends (Quaker)'\n",
      " 'United Church of Christ' \"Jehovah's Witnesses\" 'Mennonite'\n",
      " 'Christian Scientist' 'Jewish Messianic' \"Baha'I\" 'Coptic Church (Egypt)'\n",
      " 'Zoroastrian' 'Independent' 'Church of the Nazarene']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Not specified                  5483\n",
       "Roman Catholic                 2755\n",
       "Christian                      1820\n",
       "None                            839\n",
       "Methodist                       680\n",
       "Baptist                         644\n",
       "Presbyterian                    447\n",
       "Hindu                           375\n",
       "Other                           296\n",
       "Jewish                          279\n",
       "Anglican                        278\n",
       "Lutheran                        255\n",
       "Islam/Muslim                    241\n",
       "Non-Denominational              153\n",
       "Buddhism                        108\n",
       "Episcopal                        92\n",
       "Eastern Orthodox                 86\n",
       "Pentecostal                      65\n",
       "Unitarian                        42\n",
       "Protestant                       29\n",
       "Mormon-Latter Day Saints         28\n",
       "Evangelical                      24\n",
       "Sikh                             22\n",
       "Jain                             17\n",
       "Assembly of God                  15\n",
       "United Church of Christ          12\n",
       "Southern Baptist                 12\n",
       "United Methodist                 12\n",
       "Society of Friends (Quaker)       7\n",
       "Coptic Church (Egypt)             6\n",
       "Lutheran-Missouri Synod           5\n",
       "Jehovah's Witnesses               5\n",
       "Baha'I                            5\n",
       "Mennonite                         2\n",
       "Jewish Messianic                  1\n",
       "Zoroastrian                       1\n",
       "Independent                       1\n",
       "Church of the Nazarene            1\n",
       "Name: Religion, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Column 9 - Religion\n",
    "print(TU['Religion'].isna().sum())\n",
    "#5483 NAs\n",
    "print(TU['Religion'].unique())\n",
    "# no irregular categories\n",
    "# Replace NA values with not specified \n",
    "TU['Religion'].fillna(\"Not specified\", inplace=True)\n",
    "TU['Religion'].value_counts()\n",
    "\n",
    "# Combine similar levels into one level\n",
    "TU['Religion'] = \\\n",
    "TU['Religion'].apply(lambda x: 'Christian' if x in ['Bible Churches', 'Christian Reformed', 'Christian Scientist', 'Church of Christ', 'Church of God'] else x)\n",
    "TU['Religion'] = \\\n",
    "TU['Religion'].apply(lambda x: 'Presbyterian' if x == 'Presbyterian Church of America' else x)\n",
    "TU['Religion'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9057c849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Not specified         5483\n",
       "Roman Catholic        2755\n",
       "Christian             1820\n",
       "None                   839\n",
       "Methodist              680\n",
       "Baptist                644\n",
       "Others                 598\n",
       "Presbyterian           447\n",
       "Hindu                  375\n",
       "Other                  296\n",
       "Jewish                 279\n",
       "Anglican               278\n",
       "Lutheran               255\n",
       "Islam/Muslim           241\n",
       "Non-Denominational     153\n",
       "Name: Religion, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Column 9 - Religion (augments existing variables continued)\n",
    "ReligionList = list(TU['Religion'].value_counts()[:14].index)\n",
    "TU['Religion'] = \\\n",
    "TU['Religion'].apply(lambda x: 'Others' if x not in ReligionList else x)\n",
    "TU['Religion'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89ebce42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0        2016-11-18\n",
       "1        2017-01-30\n",
       "2        2019-01-31\n",
       "3        2015-06-15\n",
       "4        2018-02-19\n",
       "            ...    \n",
       "15138    2018-02-19\n",
       "15139    2018-02-16\n",
       "15140    2019-06-25\n",
       "15141    2016-02-03\n",
       "15142    2017-01-30\n",
       "Name: First_Source Origin First Source Date, Length: 15143, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Column 10 - First Source First Source Date\n",
    "print(TU['First_Source Origin First Source Date'].isna().sum())\n",
    "# no missing values\n",
    "TU['First_Source Origin First Source Date'] = pd.to_datetime(TU['First_Source Origin First Source Date'], errors='coerce').dt.date\n",
    "TU['First_Source Origin First Source Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d53efb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        2017-02-13\n",
       "1               NaT\n",
       "2               NaT\n",
       "3        2016-10-18\n",
       "4               NaT\n",
       "            ...    \n",
       "15138           NaT\n",
       "15139    2018-08-28\n",
       "15140           NaT\n",
       "15141    2017-09-10\n",
       "15142    2018-02-19\n",
       "Name: Inquiry Date, Length: 15143, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Column 11 - Inquiry Date \n",
    "# convert to date format\n",
    "TU['Inquiry Date'] = pd.to_datetime(TU['Inquiry Date'], errors='coerce').dt.date\n",
    "TU['Inquiry Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa5b81d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        2016-11-17\n",
       "1        2018-10-18\n",
       "2        2019-10-24\n",
       "3        2016-11-02\n",
       "4        2019-03-05\n",
       "            ...    \n",
       "15138    2020-01-01\n",
       "15139    2018-10-28\n",
       "15140    2019-11-01\n",
       "15141    2017-10-11\n",
       "15142    2018-10-16\n",
       "Name: Submitted, Length: 15143, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Column 12 - Submitted (same process as column 11)\n",
    "TU['Submitted'] = pd.to_datetime(TU['Submitted'], errors='coerce').dt.date\n",
    "TU['Submitted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cadaa693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Submit_FirstSource</th>\n",
       "      <th>Submit_Inquiry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>89.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>72.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15138</th>\n",
       "      <td>97.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15139</th>\n",
       "      <td>36.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15140</th>\n",
       "      <td>18.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15141</th>\n",
       "      <td>88.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15142</th>\n",
       "      <td>89.0</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15143 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Submit_FirstSource  Submit_Inquiry\n",
       "0                    -1.0           -13.0\n",
       "1                    89.0            11.0\n",
       "2                    38.0            11.0\n",
       "3                    72.0             2.0\n",
       "4                    54.0            11.0\n",
       "...                   ...             ...\n",
       "15138                97.0            11.0\n",
       "15139                36.0             8.0\n",
       "15140                18.0            11.0\n",
       "15141                88.0             4.0\n",
       "15142                89.0            34.0\n",
       "\n",
       "[15143 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Column10-12\n",
    "#After viewing Column10-12, it would be interesting to see\n",
    "#whether the differences between submission date and First_Source date\n",
    "#the differences between submission date and inquiry date affect the response.\n",
    "#So let's calculate the time difference between submission date and first_source date.\n",
    "TU['Submit_FirstSource'] = \\\n",
    "(TU['Submitted']-TU['First_Source Origin First Source Date']).astype('timedelta64[W]')\n",
    "TU['Submit_Inquiry']= \\\n",
    "(TU['Submitted']-TU['Inquiry Date']).astype('timedelta64[W]')\n",
    "#There are NAs in Inquiry.Date, \n",
    "#thus leading to NAs in Submit_Inquiry.\n",
    "#Impute NAs in Submit_Inquiry with median values.\n",
    "TU.Submit_Inquiry.fillna(TU['Submit_Inquiry'].median(),inplace=True)\n",
    "TU.Submit_Inquiry.isna().sum()\n",
    "#Remove Column10-12 in the modeling stage since they are used to construct new variables.   \n",
    "TU.drop(['First_Source Origin First Source Date','Submitted','Inquiry Date'], \\\n",
    "        axis='columns', inplace=True)\n",
    "\n",
    "TU[['Submit_FirstSource', 'Submit_Inquiry']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8cdc216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns 13 and 14- no data cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7bbf9c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column 15 - Staff Assigned Name \n",
    "TU.drop(['Staff Assigned Name'], axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "256ed1ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13658\n",
      "[nan 'Legacy' 'Legacy, Opt Out' 'Fine Arts, Legacy' 'Athlete, Legacy'\n",
      " 'Fine Arts, Legacy, VIP' 'Legacy, VIP' 'Athlete, Legacy, VIP'\n",
      " 'Athlete, Legacy, Opt Out' 'Legacy, Opt Out, VIP'\n",
      " 'Fine Arts, Legacy, Opt Out' 'Athlete, Legacy, Opt Out, VIP'\n",
      " 'Athlete, Fine Arts, Legacy' 'Fine Arts, Legacy, Opt Out, VIP'\n",
      " 'Athlete, Fine Arts, Legacy, VIP'\n",
      " 'Athlete, Fine Arts, Legacy, Opt Out, VIP']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "No Legacy          13658\n",
       "Legacy, Opt Out      852\n",
       "Legacy               633\n",
       "Name: Legacy, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Column 16 - Legacy\n",
    "print(TU['Legacy'].isna().sum())\n",
    "# 13658 missing values in data \n",
    "print(TU['Legacy'].unique())\n",
    "# No irredular categories\n",
    "# Replace missing values with \"No Legacy\"\n",
    "TU['Legacy'].fillna(\"No Legacy\", inplace=True)\n",
    "\n",
    "TU['Legacy'].value_counts() # Counts the values of all categories in this column\n",
    "TU['Legacy'] = TU['Legacy'].apply(lambda x: 'Legacy, Opt Out' if x not in ['Legacy', 'No Legacy'] else x)\n",
    "TU['Legacy'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4899bd54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13120\n",
      "[nan 'Athlete, Opt Out' 'Athlete' 'Athlete, Fine Arts' 'Athlete, Legacy'\n",
      " 'Athlete, Legacy, VIP' 'Athlete, Legacy, Opt Out' 'Athlete, VIP'\n",
      " 'Athlete, Legacy, Opt Out, VIP' 'Athlete, Fine Arts, Opt Out'\n",
      " 'Athlete, Opt Out, VIP' 'Athlete, Fine Arts, Legacy'\n",
      " 'Athlete, Fine Arts, Legacy, VIP'\n",
      " 'Athlete, Fine Arts, Legacy, Opt Out, VIP']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Non-Athlete         13120\n",
       "Athlete              1291\n",
       "Athlete, Opt Out      732\n",
       "Name: Athlete, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Column 17 - Athlete\n",
    "print(TU['Athlete'].isna().sum())\n",
    "# 13120 missing values in column\n",
    "print(TU['Athlete'].unique())\n",
    "# No irregular categories\n",
    "# fill missing values with Non-Athlete\n",
    "TU['Athlete'].fillna(\"Non-Athlete\", inplace=True)\n",
    "\n",
    "TU['Athlete'].value_counts()  # Counts all values from each category\n",
    "TU['Athlete'] = TU['Athlete'].apply(lambda x: 'Athlete, Opt Out' if x not in ['Athlete', 'Non-Athlete'] else x)\n",
    "TU['Athlete'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca9e92a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13120\n",
      "[nan 'Baseball' 'Football' 'Track Women' 'Tennis Women'\n",
      " 'Cross Country Men' 'Tennis Men' 'Soccer Men' 'Golf Women' 'Soccer Women'\n",
      " 'Basketball Men' 'Softball' 'Swimming Women' 'Cross Country Women'\n",
      " 'Track Men' 'Golf Men' 'Swimming Men' 'Volleyball' 'Basketball Women'\n",
      " 'Diving Women' 'Diving Men']\n",
      "['No Sport' 'Baseball' 'Football' 'Track ' 'Tennis ' 'Cross Country '\n",
      " 'Soccer ' 'Golf ' 'Basketball ' 'Softball' 'Swimming ' 'Volleyball'\n",
      " 'Diving ']\n"
     ]
    }
   ],
   "source": [
    "# Column 18 - Sport 1 sport\n",
    "print(TU['Sport 1 Sport'].isna().sum())\n",
    "# 13120 missing values in original column \n",
    "print(TU['Sport 1 Sport'].unique())\n",
    "# no categories that don't make sense\n",
    "# Replace missing values with \"No Sport\"\n",
    "TU['Sport 1 Sport'].fillna(\"No Sport\", inplace=True)\n",
    "\n",
    "# Group sports for males and females separately\n",
    "TU['Sport 1 Sport'] = TU['Sport 1 Sport'].str.replace('Women','')\n",
    "TU['Sport 1 Sport'] = TU['Sport 1 Sport'].str.replace('Men','')\n",
    "print(TU['Sport 1 Sport'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f027e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13120\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['No Sport', 'Franchise', 'Blue Chip', 'Varsity'], dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Column 19 - Sport 1 Rating \n",
    "print(TU['Sport 1 Rating'].isna().sum())\n",
    "# 13120 missing values in original column \n",
    "# Fill missing values with No Sport as with Column 18\n",
    "TU['Sport 1 Rating'].fillna(\"No Sport\", inplace=True)\n",
    "TU['Sport 1 Rating'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "138d8fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14513\n",
      "['No 2ndSport' '2ndSport']\n"
     ]
    }
   ],
   "source": [
    "# Column 20 - Sport 2 Sport\n",
    "print(TU['Sport 2 Sport'].isna().sum())\n",
    "#14513 missing values in Column 20\n",
    "TU['Sport 2 Sport'].fillna(\"No 2ndSport\", inplace=True)\n",
    "TU['Sport 2 Sport'].value_counts()   # counts numbers of values in each category\n",
    "TU['Sport 2 Sport'] = TU['Sport 2 Sport'].apply(lambda x: '2ndSport' if x != 'No 2ndSport' else x)\n",
    "print(TU['Sport 2 Sport'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "406e468a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15085\n",
      "[nan 'Blue Chip' 'Varsity' 'Franchise']\n"
     ]
    }
   ],
   "source": [
    "# Column 21 - Sport 2 Rating \n",
    "print(TU['Sport 2 Rating'].isna().sum())\n",
    "# 15085 missing values\n",
    "print(TU['Sport 2 Rating'].unique())\n",
    "# Remove this column in the modeling stage\n",
    "TU.drop('Sport 2 Rating', axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "da113fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14907\n",
      "['No 3rdSport' '3rdSport']\n"
     ]
    }
   ],
   "source": [
    "# Column 22 - Sport 3 Sport \n",
    "print(TU['Sport 3 Sport'].isna().sum())\n",
    "# 14907 missing values - replace with category No 3rdSport\n",
    "TU['Sport 3 Sport'].fillna(\"No 3rdSport\", inplace=True)\n",
    "TU['Sport 3 Sport'].value_counts()\n",
    "\n",
    "TU['Sport 3 Sport'] = TU['Sport 3 Sport'].apply(lambda x: \"3rdSport\" if x != \"No 3rdSport\" else x)\n",
    "print(TU['Sport 3 Sport'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2e3e0591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15140\n",
      "[nan 'Varsity']\n"
     ]
    }
   ],
   "source": [
    "# Column 23 - Sport 3 Rating \n",
    "print(TU['Sport 3 Rating'].isna().sum())\n",
    "# 15140 missing values in Column 23\n",
    "print(TU['Sport 3 Rating'].unique())\n",
    "# This column would not provide enough information. This column should be removed\n",
    "TU.drop('Sport 3 Rating', axis = 'columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a7f5b8cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "['Biology' 'Engineering Science' 'Psychology' 'Neuroscience'\n",
      " 'Computer Science' 'English' 'Mathematics' 'Education' 'Music'\n",
      " 'Undecided' 'Business' 'Geosciences' 'Pre-Law' 'Biochemistry' 'Finance'\n",
      " 'Pre-Medical' 'Political Science' 'Business - Communication Management'\n",
      " 'Economics' 'Business - Accounting' 'Entrepreneurship' 'Sociology'\n",
      " 'Environmental Studies' 'Chemistry' 'Mathematical Finance'\n",
      " 'Business - Sport Management' 'History' 'International Studies'\n",
      " 'Biochemistry & Molecular Biology' 'Business - Management'\n",
      " 'Communication' 'Anthropology' 'Business - Marketing' 'Linguistics'\n",
      " 'Philosophy' 'Business - International Business'\n",
      " 'Business Analytics & Technology' 'Art' 'French'\n",
      " 'Business - Management Information Systems' 'Physics' 'Urban Studies'\n",
      " 'Chinese' 'Nursing' 'Business Legal Studies' 'Human Communication'\n",
      " 'Comparative Literature' 'Creative Writing'\n",
      " 'Ancient Mediterranean Studies' 'Art History' 'Architectural Studies'\n",
      " 'Film Studies' 'Theatre' 'Music Education' 'Pre-Dental' 'Religion'\n",
      " 'Classical Languages' 'Applied Chemistry' 'Music Composition'\n",
      " 'Architecture' 'Choral Music' 'New Media' 'Astronomy' 'Pharmacy'\n",
      " 'African American Studies' 'East Asian Studies' nan 'Latin'\n",
      " 'Cognitive Science' \"Women's & Gender Studies\" 'Pre-Veterinary' 'Spanish'\n",
      " 'Instrumental Music' 'German' 'Russian' 'Biomathematics' 'Agriculture'\n",
      " 'Foreign Languages' 'Art and Art History' 'Italian'\n",
      " 'Global Latinx Studies' 'Arts, Letters & Enterprise']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0m/l4prbb_56q1gl8j3h3qzryb40000gn/T/ipykernel_95393/4035662498.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  TU['Academic Interest 1'][i]=TU['Academic Interest 2'][i]\n"
     ]
    }
   ],
   "source": [
    "# Column 24 - Academic Interest 1\n",
    "print(TU['Academic Interest 1'].isna().sum())\n",
    "#6 NAs.\n",
    "print(TU['Academic Interest 1'].unique())\n",
    "#Most of the NAs for Academic.Interest.1 have a value for Academic.Interest.2\n",
    "#We may assign the corresponding values in Academic.Interest.2 \n",
    "#to NAs in Academic.Interest.1 if Academic.Interest.2 has a value.\n",
    "#TU[TU['Academic Interest 1'].isna()]['Academic Interest 2']\n",
    "\n",
    "for i,b in TU[TU['Academic Interest 1'].isna()].iterrows():\n",
    "    TU['Academic Interest 1'][i]=TU['Academic Interest 2'][i]\n",
    "    \n",
    "#For the remaining NAs in Academic.Interest.1, assign Undecided.\n",
    "TU['Academic Interest 1'].fillna('Undecided', inplace=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "888bd517",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pre-Medical                            1581\n",
       "Biology                                1336\n",
       "Engineering Science                    1285\n",
       "Business                               1281\n",
       "Computer Science                        894\n",
       "Others                                  880\n",
       "Psychology                              852\n",
       "Undecided                               647\n",
       "Political Science                       624\n",
       "Neuroscience                            578\n",
       "Biochemistry & Molecular Biology        437\n",
       "Economics                               347\n",
       "Biochemistry                            334\n",
       "International Studies                   333\n",
       "Business - Management                   324\n",
       "Mathematics                             289\n",
       "English                                 288\n",
       "Business - Marketing                    285\n",
       "Business - Accounting                   274\n",
       "Pre-Law                                 255\n",
       "Chemistry                               253\n",
       "Environmental Studies                   250\n",
       "Business - International Business       240\n",
       "History                                 217\n",
       "Physics                                 216\n",
       "Communication                           178\n",
       "Education                               164\n",
       "Business - Communication Management     164\n",
       "Music                                   123\n",
       "Business Analytics & Technology         114\n",
       "Art                                     100\n",
       "Name: Academic Interest 1, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Column 24 (continued) - Group minor categories into 1\n",
    "#Group Business related options into \"Business\".\n",
    "TU['Academic Interest 1'] = \\\n",
    "TU['Academic Interest 1'].apply(lambda x: 'Business' if x in['Finance','Entrepreneurship'] else x)\n",
    "\n",
    "\n",
    "\n",
    "#Group options with a low number of cases (< 100 cases) into \"Others\".\n",
    "Majorlist=list(TU['Academic Interest 1'].value_counts()[:30].index)\n",
    "TU['Academic Interest 1'] = \\\n",
    "TU['Academic Interest 1'].apply(lambda x: 'Others' if x not in Majorlist else x)\n",
    "TU['Academic Interest 1'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4055754b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159\n",
      "['Others' 'Computer Science' 'Biology' 'Engineering Science'\n",
      " 'Biochemistry & Molecular Biology' 'Environmental Studies' 'English'\n",
      " 'Spanish' 'Psychology' 'Economics' 'Business - Marketing' 'Physics'\n",
      " 'Undecided' 'Mathematics' 'Business - Communication Management'\n",
      " 'Business - Accounting' 'International Studies' 'Neuroscience'\n",
      " 'Business Analytics & Technology' 'Business' 'Political Science'\n",
      " 'Sociology' 'Biochemistry' 'Pre-Medical'\n",
      " 'Business - International Business' 'Communication' 'Pre-Law'\n",
      " 'Business - Management' 'Anthropology' 'Business - Sport Management'\n",
      " 'Music' 'History' 'Creative Writing' 'Mathematical Finance' 'Chemistry'\n",
      " 'Education' 'Philosophy' 'Art']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Business                               1211\n",
       "Biology                                1159\n",
       "Others                                 1089\n",
       "Pre-Medical                             850\n",
       "Psychology                              774\n",
       "Undecided                               643\n",
       "Biochemistry & Molecular Biology        605\n",
       "Engineering Science                     597\n",
       "Political Science                       575\n",
       "Business - Management                   505\n",
       "Biochemistry                            502\n",
       "Economics                               490\n",
       "Computer Science                        484\n",
       "Neuroscience                            474\n",
       "Mathematics                             414\n",
       "Pre-Law                                 368\n",
       "Chemistry                               362\n",
       "Business - Marketing                    360\n",
       "Environmental Studies                   303\n",
       "International Studies                   287\n",
       "Physics                                 279\n",
       "Business - International Business       268\n",
       "English                                 264\n",
       "History                                 236\n",
       "Sociology                               226\n",
       "Business - Accounting                   206\n",
       "Education                               201\n",
       "Spanish                                 163\n",
       "Business Analytics & Technology         156\n",
       "Music                                   147\n",
       "Business - Communication Management     140\n",
       "Philosophy                              130\n",
       "Communication                           122\n",
       "Business - Sport Management             120\n",
       "Art                                     113\n",
       "Creative Writing                        111\n",
       "Anthropology                            105\n",
       "Mathematical Finance                    104\n",
       "Name: Academic Interest 2, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Column 25 - Academic Interest 2\n",
    "print(TU['Academic Interest 2'].isna().sum())\n",
    "#159 NAs.\n",
    "#Replace repeated academic interests with Undecided, \n",
    "#then make NAs Undecided\n",
    "TU['Academic Interest 2'].fillna('Undecided', inplace=True)\n",
    "\n",
    "#Group Business related options into \"Business\".\n",
    "TU['Academic Interest 2'] = \\\n",
    "TU['Academic Interest 2'].apply(lambda x: 'Business' if x in['Finance','Entrepreneurship'] else x)\n",
    "\n",
    "\n",
    "#Group options with a low number of cases (< 100 cases) into \"Others\".\n",
    "Majorlist=list(TU['Academic Interest 2'].value_counts()[:37].index)\n",
    "TU['Academic Interest 2'] = \\\n",
    "TU['Academic Interest 2'].apply(lambda x: 'Others' if x not in Majorlist else x)\n",
    "print(TU['Academic Interest 2'].unique())\n",
    "TU['Academic Interest 2'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "67d548b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "['CAP' 'CBINQ' 'OEVNT' 'OAPP' 'PSAT' 'CF' 'SATR' 'HSV' 'WEBCA' 'HOBS'\n",
      " 'VST' 'YUVST' 'SRCH' 'WEBTU' 'ACTPL' 'ACT' 'ATH' 'SIB' 'APPTX' 'CAPIQ'\n",
      " 'NICHE' 'CLNIQ' 'TVINT' 'NHI' 'SAT' 'AP' 'TIF' 'TVOTH' 'OTH' 'ATHWB'\n",
      " 'DOC' 'ALUM' 'GRP' 'REF' 'EM' 'DBT' 'RCPT' 'CHEGG' 'MPC' 'MAIL' 'CLNAP'\n",
      " 'TFL' 'ATS' 'EXPL' 'LVCHT' 'HIGH' 'TEL' 'DUOL' 'WEBOT' 'APCU']\n",
      "['CAP' 'CBINQ' 'OEVNT' 'OAPP' 'PSAT' 'CF' 'SATR' 'HSV' 'Others' 'VST'\n",
      " 'YUVST' 'SRCH' 'WEBTU' 'ACT' 'ATH' 'SIB' 'CAPIQ' 'TIF']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CBINQ     7070\n",
       "OAPP      1521\n",
       "SRCH       764\n",
       "PSAT       755\n",
       "Others     748\n",
       "VST        675\n",
       "CF         551\n",
       "WEBTU      513\n",
       "CAPIQ      463\n",
       "CAP        426\n",
       "ACT        284\n",
       "HSV        255\n",
       "ATH        218\n",
       "TIF        194\n",
       "SATR       190\n",
       "SIB        186\n",
       "YUVST      178\n",
       "OEVNT      152\n",
       "Name: First_Source Origin First Source Summary, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Column 26 - First_Source Origin First Source Summary\n",
    "print(TU['First_Source Origin First Source Summary'].isna().sum())\n",
    "#No NAs.\n",
    "print(TU['First_Source Origin First Source Summary'].unique())\n",
    "#No irregular values\n",
    "\n",
    "#Group options with a low number of cases (< 100) into \"Other Sources\".\n",
    "Majorlist=list(TU['First_Source Origin First Source Summary'].value_counts()[:17].index)\n",
    "\n",
    "TU['First_Source Origin First Source Summary'] = \\\n",
    "TU['First_Source Origin First Source Summary'].apply(lambda x: 'Others' if x not in Majorlist else x)\n",
    "print(TU['First_Source Origin First Source Summary'].unique())\n",
    "TU['First_Source Origin First Source Summary'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c8cefdb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[0 1 2 4 3 5]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, '2 or more'], dtype=object)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Column 27 - Total Event Participation\n",
    "print(TU['Total Event Participation'].isna().sum())\n",
    "#No NAs.\n",
    "print(TU['Total Event Participation'].unique())\n",
    "\n",
    "#3, 4, 5 combined accounts for < 1% of the data set.\n",
    "#Compared to the number of cases in 0, 1, and 2, the number of cases\n",
    "#in 3, 4, and 5 won't be very useful in predicting the response.\n",
    "#So group 3, 4, and 5 into \"2 or more\".\n",
    "\n",
    "TU['Total Event Participation']=\\\n",
    "TU['Total Event Participation'].apply(lambda x: '2 or more' if x in[3,4,5] else x)\n",
    "TU['Total Event Participation'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "813408ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[5 0 2 1 3 4 6 8]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['4 or more', 0, 2, 1, 3, 4], dtype=object)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Column 28 - Count of Campus Visits\n",
    "print(TU['Count of Campus Visits'].isna().sum())\n",
    "#No NAs.\n",
    "print(TU['Count of Campus Visits'].unique())\n",
    "# group 5, 6, and 8 into '4 or more'.\n",
    "TU['Count of Campus Visits']= TU['Count of Campus Visits'].apply(lambda x: '4 or more' if x in [5,6,8] else x)\n",
    "TU['Count of Campus Visits'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4229dceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n",
      "['High School' nan 'College']\n"
     ]
    }
   ],
   "source": [
    "# Column 29 - School 1 Organization Category \n",
    "print(TU['School #1 Organization Category'].isna().sum())\n",
    "#38  NAs.\n",
    "print(TU['School #1 Organization Category'].unique())\n",
    "#Only 16 cases belong to College but 15089 cases belong to High School.\n",
    "#Remove this variable.\n",
    "TU.drop('School #1 Organization Category', axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "60372112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11897\n",
      "[    nan 441750. 390324. ... 447335. 263055.  52117.]\n"
     ]
    }
   ],
   "source": [
    "# Column 30 - School 1 Code\n",
    "print(TU['School 1 Code'].isna().sum())\n",
    "#11879 NAs.\n",
    "print(TU['School 1 Code'].unique())\n",
    "#School Code will not matter much to produce insightful information.\n",
    "#Additionally, there are 11879 missing values.\n",
    "#so remove this column in the modeling stage.\n",
    "TU.drop('School 1 Code', axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8b8299ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8136\n"
     ]
    }
   ],
   "source": [
    "# Column 31 - School 1 Class Rank (Numeric)\n",
    "print(TU['School 1 Class Rank (Numeric)'].isna().sum())\n",
    "#8136 NAs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0d9abcad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8136\n",
      "8136\n",
      "829\n",
      "[ 3.  2.  4.  1.  5. nan]\n",
      "[3. 2. 4. 1. 5.]\n"
     ]
    }
   ],
   "source": [
    "# Column 32 - School 1 Class Size (Numeric)\n",
    "print(TU['School 1 Class Size (Numeric)'].isna().sum())\n",
    "\n",
    "#8136 NAs.\n",
    "#Percentage rank can more accurately reflect a student's academic performance\n",
    "#than numeric rank. \n",
    "\n",
    "#New Column - School 1 Top Percent in Class\n",
    "\n",
    "TU['School 1 Top Percent in Class'] =\\\n",
    "100 *(TU['School 1 Class Rank (Numeric)']/TU['School 1 Class Size (Numeric)'])\n",
    "#remove Column31 and Column32\n",
    "TU.drop(['School 1 Class Rank (Numeric)','School 1 Class Size (Numeric)'], axis='columns',\n",
    "       inplace=True)\n",
    "\n",
    "print(TU['School 1 Top Percent in Class'].isna().sum())\n",
    "# #Impute the 8136 NAs based on Academic.Index column. \n",
    "# #Since we need to handle NAs in School.1.Top.Percent.in.Class\n",
    "# #according Academic.Index, first let's see whether Academic Index needs to cleaned.\n",
    "print(TU['Academic Index'].isna().sum())\n",
    "#829 NAs.\n",
    "print(TU['Academic Index'].unique())\n",
    "#No questionable level.\n",
    "#Impute 829 NAs with the most common level.\n",
    "TU['Academic Index'].fillna(3,inplace=True)\n",
    "print(TU['Academic Index'].unique())\n",
    "#No missing values in Academic Index now.\n",
    "#Impute missing values in School 1 Top Percent in Class based on Academic.Index.\n",
    "\n",
    "# clean_data <- rbind(clean_index_1, clean_index_2, clean_index_3, clean_index_4, clean_index_5)\n",
    "# #Remove index dataframes\n",
    "# rm(clean_index_1, clean_index_2, clean_index_3, clean_index_4, clean_index_5)\n",
    "# #Later when implementing KNN method, I will use mutate()\n",
    "# #function to convert categorical variables to numeric variables, which ignores\n",
    "# #grouping variables. Therefore, to ensure Academic.Index will be converted into\n",
    "# #a numeric variable, I will ungroup clean_data.\n",
    "# clean_data <- clean_data %>% ungroup()\n",
    "# clean_data$Academic.Index <- as.factor(clean_data$Academic.Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3561c7ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Academic Index\n",
       "1.0     4.560086\n",
       "2.0     8.519419\n",
       "3.0    16.224847\n",
       "4.0    29.182240\n",
       "5.0    34.850230\n",
       "Name: School 1 Top Percent in Class, dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped=TU.groupby('Academic Index')\n",
    "average=grouped.mean('School 1 Top Percent in Class')\n",
    "average['School 1 Top Percent in Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1fdf2c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0m/l4prbb_56q1gl8j3h3qzryb40000gn/T/ipykernel_95393/2428932017.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  TU['School 1 Top Percent in Class'][i]= average['School 1 Top Percent in Class'][4.0]\n",
      "/var/folders/0m/l4prbb_56q1gl8j3h3qzryb40000gn/T/ipykernel_95393/2428932017.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  TU['School 1 Top Percent in Class'][i]= average['School 1 Top Percent in Class'][1.0]\n",
      "/var/folders/0m/l4prbb_56q1gl8j3h3qzryb40000gn/T/ipykernel_95393/2428932017.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  TU['School 1 Top Percent in Class'][i]= average['School 1 Top Percent in Class'][2.0]\n",
      "/var/folders/0m/l4prbb_56q1gl8j3h3qzryb40000gn/T/ipykernel_95393/2428932017.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  TU['School 1 Top Percent in Class'][i]= average['School 1 Top Percent in Class'][3.0]\n",
      "/var/folders/0m/l4prbb_56q1gl8j3h3qzryb40000gn/T/ipykernel_95393/2428932017.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  TU['School 1 Top Percent in Class'][i]= average['School 1 Top Percent in Class'][5.0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "for i,b in TU.iterrows():\n",
    "   # print(i,b)\n",
    "    if (TU['Academic Index'][i]== 1.0) & (pd.isna(TU['School 1 Top Percent in Class'][i])):\n",
    "        TU['School 1 Top Percent in Class'][i]= average['School 1 Top Percent in Class'][1.0]\n",
    "    elif (TU['Academic Index'][i]== 2.0) & (pd.isna(TU['School 1 Top Percent in Class'][i])):\n",
    "            TU['School 1 Top Percent in Class'][i]= average['School 1 Top Percent in Class'][2.0]\n",
    "    elif (TU['Academic Index'][i]== 3.0) & (pd.isna(TU['School 1 Top Percent in Class'][i])):\n",
    "            TU['School 1 Top Percent in Class'][i]= average['School 1 Top Percent in Class'][3.0]\n",
    "    elif (TU['Academic Index'][i]== 4.0) & (pd.isna(TU['School 1 Top Percent in Class'][i])):\n",
    "            TU['School 1 Top Percent in Class'][i]= average['School 1 Top Percent in Class'][4.0]\n",
    "    elif (TU['Academic Index'][i]== 5.0) & (pd.isna(TU['School 1 Top Percent in Class'][i])):\n",
    "            TU['School 1 Top Percent in Class'][i]= average['School 1 Top Percent in Class'][5.0]\n",
    "\n",
    "print(TU['School 1 Top Percent in Class'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e4a83197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column 33 - School 1 GPA\n",
    "# This column is irrelevant, so remove from Data frame\n",
    "TU.drop('School 1 GPA', axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "899343d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column 34 - School 1 GPA Scale \n",
    "# This column is irrelevant, so remove from Data frame\n",
    "TU.drop('School 1 GPA Scale', axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "edce4aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.929959273527639"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Column 35 - School 1 GPA Recalculated \n",
    "print(TU['School 1 GPA Recalculated'].isna().sum())\n",
    "#0 NAs.\n",
    "TU['School 1 GPA Recalculated'].skew()\n",
    "#Check skewness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e874bff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15143\n"
     ]
    }
   ],
   "source": [
    "# Column 36 - School 2 Class Rank\n",
    "# Same as column 31\n",
    "print(TU['School 2 Class Rank (Numeric)'].isna().sum())\n",
    "#15143 NAs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ce4c7fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15143\n",
      "15143\n",
      "0\n",
      "[3. 2. 4. 1. 5.]\n",
      "[3. 2. 4. 1. 5.]\n"
     ]
    }
   ],
   "source": [
    "# Column 37 - School 2 Class Size\n",
    "# Same thing as Column 32 \n",
    "print(TU['School 2 Class Size (Numeric)'].isna().sum())\n",
    "\n",
    "#8136 NAs.\n",
    "#Percentage rank can more accurately reflect a student's academic performance\n",
    "#than numeric rank. \n",
    "\n",
    "#New Column - School 1 Top Percent in Class\n",
    "\n",
    "TU['School 2 Top Percent in Class'] =\\\n",
    "100 *(TU['School 2 Class Rank (Numeric)']/TU['School 2 Class Size (Numeric)'])\n",
    "#remove Column31 and Column32\n",
    "TU.drop(['School 2 Class Rank (Numeric)','School 2 Class Size (Numeric)'], axis='columns',\n",
    "       inplace=True)\n",
    "\n",
    "print(TU['School 2 Top Percent in Class'].isna().sum())\n",
    "# #Impute the 8136 NAs based on Academic.Index column. \n",
    "# #Since we need to handle NAs in School.1.Top.Percent.in.Class\n",
    "# #according Academic.Index, first let's see whether Academic Index needs to cleaned.\n",
    "print(TU['Academic Index'].isna().sum())\n",
    "#829 NAs.\n",
    "print(TU['Academic Index'].unique())\n",
    "#No questionable level.\n",
    "#Impute 829 NAs with the most common level.\n",
    "TU['Academic Index'].fillna(3,inplace=True)\n",
    "print(TU['Academic Index'].unique())\n",
    "#No missing values in Academic Index now.\n",
    "#Impute missing values in School 1 Top Percent in Class based on Academic.Index.\n",
    "\n",
    "# clean_data <- rbind(clean_index_1, clean_index_2, clean_index_3, clean_index_4, clean_index_5)\n",
    "# #Remove index dataframes\n",
    "# rm(clean_index_1, clean_index_2, clean_index_3, clean_index_4, clean_index_5)\n",
    "# #Later when implementing KNN method, I will use mutate()\n",
    "# #function to convert categorical variables to numeric variables, which ignores\n",
    "# #grouping variables. Therefore, to ensure Academic.Index will be converted into\n",
    "# #a numeric variable, I will ungroup clean_data.\n",
    "# clean_data <- clean_data %>% ungroup()\n",
    "# clean_data$Academic.Index <- as.factor(clean_data$Academic.Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "56a19195",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Academic Index\n",
       "1.0     4.560086\n",
       "2.0     8.519419\n",
       "3.0    16.224847\n",
       "4.0    29.182240\n",
       "5.0    34.850230\n",
       "Name: School 1 Top Percent in Class, dtype: float64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped=TU.groupby('Academic Index')\n",
    "average=grouped.mean('School 1 Top Percent in Class')\n",
    "average['School 1 Top Percent in Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "50c0c473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "for i,b in TU.iterrows():\n",
    "   # print(i,b)\n",
    "    if (TU['Academic Index'][i]== 1.0) & (pd.isna(TU['School 1 Top Percent in Class'][i])):\n",
    "        TU['School 1 Top Percent in Class'][i]= average['School 1 Top Percent in Class'][1.0]\n",
    "    elif (TU['Academic Index'][i]== 2.0) & (pd.isna(TU['School 1 Top Percent in Class'][i])):\n",
    "            TU['School 1 Top Percent in Class'][i]= average['School 1 Top Percent in Class'][2.0]\n",
    "    elif (TU['Academic Index'][i]== 3.0) & (pd.isna(TU['School 1 Top Percent in Class'][i])):\n",
    "            TU['School 1 Top Percent in Class'][i]= average['School 1 Top Percent in Class'][3.0]\n",
    "    elif (TU['Academic Index'][i]== 4.0) & (pd.isna(TU['School 1 Top Percent in Class'][i])):\n",
    "            TU['School 1 Top Percent in Class'][i]= average['School 1 Top Percent in Class'][4.0]\n",
    "    elif (TU['Academic Index'][i]== 5.0) & (pd.isna(TU['School 1 Top Percent in Class'][i])):\n",
    "            TU['School 1 Top Percent in Class'][i]= average['School 1 Top Percent in Class'][5.0]\n",
    "\n",
    "print(TU['School 1 Top Percent in Class'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f542b327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column 38 - School 2 GPA\n",
    "# Same issues as in Column 33\n",
    "# This column is irrelevant \n",
    "TU.drop('School 2 GPA', axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ea7eeb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column 39 - School 2 GPA Scale \n",
    "# Same thing as in Column 34, drop because this column is irrelevant \n",
    "TU.drop('School 2 GPA Scale', axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "eff1c3ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15143\n"
     ]
    }
   ],
   "source": [
    "# Column 40 - School 2 GPA Recalculated \n",
    "print(TU['School 2 GPA Recalculated'].isna().sum())\n",
    "#15143 NAs.\n",
    "TU['School 2 GPA Recalculated'].skew()\n",
    "#Check skewness\n",
    "# remove this column: as everything is blank\n",
    "TU.drop('School 2 GPA Recalculated', axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "796327da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15143\n"
     ]
    }
   ],
   "source": [
    "# Column 41 - School 3 Class Rank \n",
    "print(TU['School 3 Class Rank (Numeric)'].isna().sum())\n",
    "\n",
    "# Remove this variable as everything is blank making it irrelevant\n",
    "TU.drop('School 3 Class Rank (Numeric)', axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fcb0fc3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15143\n"
     ]
    }
   ],
   "source": [
    "# Column 42 - School 3 Class Size\n",
    "print(TU['School 3 Class Size (Numeric)'].isna().sum())\n",
    "# 15143 NAs\n",
    "# This column is irrelevant, so remove\n",
    "TU.drop('School 3 Class Size (Numeric)', axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ff7ff05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15143\n"
     ]
    }
   ],
   "source": [
    "# Column 43 - School 3 GPA\n",
    "print(TU['School 3 GPA'].isna().sum())\n",
    "# All points in this column are blank, making this column irrelevant. Remove!\n",
    "TU.drop('School 3 GPA', axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b87ab978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15143\n"
     ]
    }
   ],
   "source": [
    "# Column 44 - School 3 GPA Scale \n",
    "print(TU['School 3 GPA Scale'].isna().sum())\n",
    "# 15143 NAs. Since this column has all points blank, remove this column as it is extraneous\n",
    "TU.drop('School 3 GPA Scale', axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "80ba22a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15143\n"
     ]
    }
   ],
   "source": [
    "# Column 45 - School 3 GPA Recalculated\n",
    "print(TU['School 3 GPA Recalculated'].isna().sum())\n",
    "# 15143 NAs. Since this column has all points blank, remove this column as it is extraneous\n",
    "TU.drop('School 3 GPA Recalculated', axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ef0820e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7883\n"
     ]
    }
   ],
   "source": [
    "# ACT English column clean up\n",
    "print(TU['ACT English'].isna().sum())\n",
    "# 7883 NAs\n",
    "# Since ACT Composite score is generally a good indicator of ACT scores, scores on each section don't matter in data analysis. Remove this column\n",
    "TU.drop('ACT English', axis='columns',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "041cbadc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7883\n"
     ]
    }
   ],
   "source": [
    "# ACT Reading column data clean up\n",
    "print(TU['ACT Reading'].isna().sum())\n",
    "# 7883 NAs\n",
    "# Since ACT composite scores are a good indicator of ACT scores, scores on each section are irrelevant in the data analysis. Remove this column\n",
    "TU.drop('ACT Reading', axis='columns', inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b62b2799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7883\n"
     ]
    }
   ],
   "source": [
    "# ACT Math scores data clean up\n",
    "print(TU['ACT Math'].isna().sum())\n",
    "#7883 NAs\n",
    "# Because ACT composite scores are a good indicator for ACT scores, scores on specific sections are irrelevant in the data analysis (remove)\n",
    "TU.drop('ACT Math', axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "09d02d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7883\n"
     ]
    }
   ],
   "source": [
    "# ACT Science Reasoning data clean up\n",
    "print(TU['ACT Science Reasoning'].isna().sum())\n",
    "# 7883 NAs\n",
    "# Because ACT composite scores are an indicator for ACT scores, scores on specific sections are irrelevant in the data analysis\n",
    "TU.drop('ACT Science Reasoning', axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c6832f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14886\n"
     ]
    }
   ],
   "source": [
    "# ACT writing data clean up \n",
    "print(TU['ACT Writing'].isna().sum())\n",
    "# 14886 NAs\n",
    "# Because ACT composite scores are an indicator for ACT scores, scores on specific sections are irrelevant in the data analysis\n",
    "TU.drop('ACT Writing', axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d96d8158",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14569\n"
     ]
    }
   ],
   "source": [
    "# SAT I CR + M data cleanup\n",
    "print(TU['SAT I CR + M'].isnull().sum())\n",
    "# 14569 NAs\n",
    "# The column is irrelevant,so remove the column\n",
    "TU.drop('SAT I CR + M', axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a728f7e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8332\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ACT Composite</th>\n",
       "      <th>SAT R</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30.0</td>\n",
       "      <td>1380.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29.0</td>\n",
       "      <td>1350.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15138</th>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15139</th>\n",
       "      <td>33.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15140</th>\n",
       "      <td>26.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15141</th>\n",
       "      <td>34.0</td>\n",
       "      <td>1510.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15142</th>\n",
       "      <td>31.0</td>\n",
       "      <td>1290.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15143 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ACT Composite   SAT R\n",
       "0               30.0    None\n",
       "1               30.0  1380.0\n",
       "2               29.0    None\n",
       "3               35.0    None\n",
       "4               29.0  1350.0\n",
       "...              ...     ...\n",
       "15138            NaN    None\n",
       "15139           33.0    None\n",
       "15140           26.0    None\n",
       "15141           34.0  1510.0\n",
       "15142           31.0  1290.0\n",
       "\n",
       "[15143 rows x 2 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SAT R cleanup\n",
    "TU['SAT R'] = TU['SAT R Evidence-Based Reading and Writing Section'] + TU['SAT R Math Section']  # SAT R represents the SAT composite score, the sum of the Evidence-Based Reading and Writing Score and the Math Score\n",
    "TU['SAT R']\n",
    "# handle missing values\n",
    "print(TU['SAT R'].isna().sum())\n",
    "# 8332 NAs - fill nas with not specified values\n",
    "TU['SAT R'].fillna(\"None\", inplace= True)\n",
    "# TO do: SAT R generating ACT scores\n",
    "def sat_to_act(sat):\n",
    "    try: \n",
    "        sat = int(sat)\n",
    "    except:\n",
    "        return None\n",
    "    sat_cutoffs = [1570, 1530, 1490, 1450, 1420, 1390, 1360, 1330, 1300, 1260,1230,1200,1160,1130,1100,1060,1030,990,960,920,880,830,780,730,690,650,620,590]\n",
    "    for i in range(len(sat_cutoffs)):\n",
    "        if sat >= sat_cutoffs[i]:\n",
    "            return 36-i\n",
    "    return None \n",
    "# TODO: comparing SAT values in the data frame in the function\n",
    "\n",
    "TU['ACT Composite'] = TU['ACT Composite'].fillna(TU['SAT R'].apply(sat_to_act))\n",
    "TU[['ACT Composite', 'SAT R']]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f08cfd68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        30.0\n",
       "1        30.0\n",
       "2        29.0\n",
       "3        35.0\n",
       "4        29.0\n",
       "         ... \n",
       "15138    30.0\n",
       "15139    33.0\n",
       "15140    26.0\n",
       "15141    34.0\n",
       "15142    31.0\n",
       "Name: ACT Composite, Length: 15143, dtype: float64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace missing concordance scores with mean of ACT concordance scores\n",
    "\n",
    "mean_ACT = round((pd.to_numeric(TU['ACT Composite'], errors = \"coerce\")).mean(),0)\n",
    "mean_ACT\n",
    "   \n",
    "TU['ACT Composite'] = TU['ACT Composite'].apply(lambda x: mean_ACT if x == \"None\" else x)\n",
    "TU['ACT Composite'].fillna(mean_ACT, inplace = True)\n",
    "TU['ACT Composite']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "60731d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "                                     Region\n",
      "State                                      \n",
      "Alabama           South, East South Central\n",
      "Alaska                        West, Pacific\n",
      "Arizona                      West, Mountain\n",
      "Arkansas          South, West South Central\n",
      "California                    West, Pacific\n",
      "Colorado                     West, Mountain\n",
      "Connecticut          Northeast, New England\n",
      "Delaware              South, South Atlantic\n",
      "D.C.                  South, South Atlantic\n",
      "Florida               South, South Atlantic\n",
      "Georgia               South, South Atlantic\n",
      "Hawaii                        West, Pacific\n",
      "Idaho                        West, Mountain\n",
      "Illinois        Midwest, East North Central\n",
      "Indiana         Midwest, East North Central\n",
      "Iowa            Midwest, West North Central\n",
      "Kansas          Midwest, West North Central\n",
      "Kentucky          South, East South Central\n",
      "Louisiana         South, West South Central\n",
      "Maine                Northeast, New England\n",
      "Maryland              South, South Atlantic\n",
      "Massachusetts        Northeast, New England\n",
      "Michigan        Midwest, East North Central\n",
      "Minnesota       Midwest, West North Central\n",
      "Mississippi       South, East South Central\n",
      "Missouri        Midwest, West North Central\n",
      "Montana                      West, Mountain\n",
      "Nebraska        Midwest, West North Central\n",
      "Nevada                       West, Mountain\n",
      "New Hampshire        Northeast, New England\n",
      "New Jersey       Northeast, Middle Atlantic\n",
      "New Mexico                   West, Mountain\n",
      "New York         Northeast, Middle Atlantic\n",
      "North Carolina        South, South Atlantic\n",
      "North Dakota    Midwest, West North Central\n",
      "Ohio            Midwest, East North Central\n",
      "Oklahoma          South, West South Central\n",
      "Oregon                        West, Pacific\n",
      "Pennsylvania     Northeast, Middle Atlantic\n",
      "Rhode Island         Northeast, New England\n",
      "South Carolina        South, South Atlantic\n",
      "South Dakota    Midwest, West North Central\n",
      "Tennessee         South, East South Central\n",
      "Texas             South, West South Central\n",
      "Utah                         West, Mountain\n",
      "Vermont              Northeast, New England\n",
      "Virginia              South, South Atlantic\n",
      "Washington                    West, Pacific\n",
      "West Virginia         South, South Atlantic\n",
      "Wisconsin       Midwest, East North Central\n",
      "Wyoming                      West, Mountain\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0        NM\n",
       "1        TX\n",
       "2        TX\n",
       "3        WA\n",
       "4        TX\n",
       "         ..\n",
       "15138    CA\n",
       "15139    TX\n",
       "15140    MN\n",
       "15141    TX\n",
       "15142    TX\n",
       "Name: State, Length: 15143, dtype: object"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Permanent Geomarket data cleanup\n",
    "print(TU['Permanent Geomarket'].isna().sum())\n",
    "# 1 NA \n",
    "# Find most frequent geomarket value\n",
    "TU['Permanent Geomarket'].fillna(TU['Permanent Geomarket'].value_counts().idxmax(), inplace = True)\n",
    "\n",
    "# Recategorize Geomarkets into regions - TODO: Getting pandas to read excel files (couldn't get it to read excel files)\n",
    "Regions = pd.read_excel(\"Regions.xlsx\", sheet_name = \"State\", index_col=\"State\")\n",
    "print(Regions)\n",
    "Regions.get('State')\n",
    "\n",
    "TU['Permanent Geomarket']\n",
    "\n",
    "# Due to the outputs of Permanent Geomarket not matching Regions, we have to get the state first\n",
    "TU[['State', 'code']] = TU['Permanent Geomarket'].str.split('-', expand=True)\n",
    "TU[['State', 'code']]\n",
    "TU['State']\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "119ae9a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        New Mexico\n",
       "1             Texas\n",
       "2             Texas\n",
       "3        Washington\n",
       "4             Texas\n",
       "            ...    \n",
       "15138    California\n",
       "15139         Texas\n",
       "15140     Minnesota\n",
       "15141         Texas\n",
       "15142         Texas\n",
       "Name: State, Length: 15143, dtype: object"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = {'AL':'Alabama', 'AK':'Alaska', 'AZ': 'Arizona', 'AR': 'Arkansas', 'CA': 'California', \n",
    "              'CO':'Colorado', 'CT': 'Connecticut', 'DE': 'Delaware', 'DC': 'D.C.', 'FL': 'Florida', 'GA': 'Georgia', \n",
    "             'HI': 'Hawaii', 'ID': 'Idaho', 'IL': 'Illinois', 'IN': 'Indiana', 'IA': 'Iowa', 'KS':'Kansas',\n",
    "             'KY': 'Kentucky', 'LA': 'Louisiana', 'MA': 'Massachusetts', 'MI':'Michigan','ME': 'Maine', 'MD': 'Maryland', 'MN': 'Minnesota', 'MO':'Missouri', 'MS': 'Mississippi', \n",
    "             'MT':'Montana','NE': 'Nebraska', 'NV': 'Nevada', 'NH': 'New Hampshire', 'NJ': 'New Jersey', 'NM': 'New Mexico', 'NY':'New York',\n",
    "             'NC':'North Carolina', 'ND': 'North Dakota', 'OH': 'Ohio', 'OK': 'Oklahoma', 'OR': 'Oregon', 'PA': 'Pennsylvania',\n",
    "             'RI':'Rhode Island', 'SC':'South Carolina', 'SD': 'South Dakota', 'TN': 'Tennessee', 'TX':'Texas', 'UT': 'Utah',\n",
    "             'VT':'Vermont', 'VA': 'Virginia', 'WA': 'Washington', 'WV': 'West Virginia', 'WI': 'Wisconsin', 'WY': 'Wyoming'}\n",
    "\n",
    "TU['State'].replace(state_dict, inplace = True)\n",
    "TU['State']     # State column after manipulation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "57622353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>train-test</th>\n",
       "      <th>Entry Term (Application)</th>\n",
       "      <th>Permanent Country</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Ethnicity</th>\n",
       "      <th>Race</th>\n",
       "      <th>Religion</th>\n",
       "      <th>Application Source</th>\n",
       "      <th>Decision Plan</th>\n",
       "      <th>...</th>\n",
       "      <th>SAT R Math Section</th>\n",
       "      <th>Decision</th>\n",
       "      <th>Submit_FirstSource</th>\n",
       "      <th>Submit_Inquiry</th>\n",
       "      <th>School 1 Top Percent in Class</th>\n",
       "      <th>School 2 Top Percent in Class</th>\n",
       "      <th>SAT R</th>\n",
       "      <th>State</th>\n",
       "      <th>code</th>\n",
       "      <th>Region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>Fall 2017</td>\n",
       "      <td>United States</td>\n",
       "      <td>F</td>\n",
       "      <td>Non Hispanic/Latino</td>\n",
       "      <td>White</td>\n",
       "      <td>Roman Catholic</td>\n",
       "      <td>CommonApp</td>\n",
       "      <td>Early Action II</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-13.0</td>\n",
       "      <td>10.094637</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>New Mexico</td>\n",
       "      <td>01</td>\n",
       "      <td>West, Mountain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>train</td>\n",
       "      <td>Fall 2019</td>\n",
       "      <td>United States</td>\n",
       "      <td>M</td>\n",
       "      <td>Non Hispanic/Latino</td>\n",
       "      <td>White</td>\n",
       "      <td>Not specified</td>\n",
       "      <td>CommonApp</td>\n",
       "      <td>Early Action I</td>\n",
       "      <td>...</td>\n",
       "      <td>720.0</td>\n",
       "      <td>0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>13.878470</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1380.0</td>\n",
       "      <td>Texas</td>\n",
       "      <td>22</td>\n",
       "      <td>South, West South Central</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>train</td>\n",
       "      <td>Fall 2020</td>\n",
       "      <td>United States</td>\n",
       "      <td>F</td>\n",
       "      <td>Hispanic/Latino</td>\n",
       "      <td>White</td>\n",
       "      <td>Christian</td>\n",
       "      <td>ApplyTexas</td>\n",
       "      <td>Early Action</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>29.182240</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>Texas</td>\n",
       "      <td>15</td>\n",
       "      <td>South, West South Central</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>train</td>\n",
       "      <td>Fall 2017</td>\n",
       "      <td>United States</td>\n",
       "      <td>F</td>\n",
       "      <td>Non Hispanic/Latino</td>\n",
       "      <td>Asian</td>\n",
       "      <td>Not specified</td>\n",
       "      <td>CommonApp</td>\n",
       "      <td>Early Action I</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.112150</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>Washington</td>\n",
       "      <td>05</td>\n",
       "      <td>West, Pacific</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>train</td>\n",
       "      <td>Fall 2019</td>\n",
       "      <td>United States</td>\n",
       "      <td>M</td>\n",
       "      <td>Non Hispanic/Latino</td>\n",
       "      <td>White</td>\n",
       "      <td>Christian</td>\n",
       "      <td>CommonApp</td>\n",
       "      <td>Regular Decision</td>\n",
       "      <td>...</td>\n",
       "      <td>700.0</td>\n",
       "      <td>0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>18.601583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1350.0</td>\n",
       "      <td>Texas</td>\n",
       "      <td>06</td>\n",
       "      <td>South, West South Central</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15138</th>\n",
       "      <td>15139</td>\n",
       "      <td>test</td>\n",
       "      <td>Fall 2020</td>\n",
       "      <td>United States</td>\n",
       "      <td>F</td>\n",
       "      <td>Non Hispanic/Latino</td>\n",
       "      <td>Asian</td>\n",
       "      <td>Not specified</td>\n",
       "      <td>CommonApp</td>\n",
       "      <td>Regular Decision</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>16.224847</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>California</td>\n",
       "      <td>16</td>\n",
       "      <td>West, Pacific</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15139</th>\n",
       "      <td>15140</td>\n",
       "      <td>test</td>\n",
       "      <td>Fall 2019</td>\n",
       "      <td>United States</td>\n",
       "      <td>F</td>\n",
       "      <td>Non Hispanic/Latino</td>\n",
       "      <td>Asian</td>\n",
       "      <td>Hindu</td>\n",
       "      <td>CommonApp</td>\n",
       "      <td>Early Action I</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.422111</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>Texas</td>\n",
       "      <td>16</td>\n",
       "      <td>South, West South Central</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15140</th>\n",
       "      <td>15141</td>\n",
       "      <td>test</td>\n",
       "      <td>Fall 2020</td>\n",
       "      <td>United States</td>\n",
       "      <td>M</td>\n",
       "      <td>Non Hispanic/Latino</td>\n",
       "      <td>Black or African American</td>\n",
       "      <td>Not specified</td>\n",
       "      <td>CommonApp</td>\n",
       "      <td>Early Action</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>29.182240</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>Minnesota</td>\n",
       "      <td>01</td>\n",
       "      <td>Midwest, West North Central</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15141</th>\n",
       "      <td>15142</td>\n",
       "      <td>test</td>\n",
       "      <td>Fall 2018</td>\n",
       "      <td>United States</td>\n",
       "      <td>F</td>\n",
       "      <td>Non Hispanic/Latino</td>\n",
       "      <td>White</td>\n",
       "      <td>Not specified</td>\n",
       "      <td>CommonApp</td>\n",
       "      <td>Early Action I</td>\n",
       "      <td>...</td>\n",
       "      <td>740.0</td>\n",
       "      <td>0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.785714</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1510.0</td>\n",
       "      <td>Texas</td>\n",
       "      <td>23</td>\n",
       "      <td>South, West South Central</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15142</th>\n",
       "      <td>15143</td>\n",
       "      <td>test</td>\n",
       "      <td>Fall 2019</td>\n",
       "      <td>United States</td>\n",
       "      <td>F</td>\n",
       "      <td>Non Hispanic/Latino</td>\n",
       "      <td>White</td>\n",
       "      <td>Roman Catholic</td>\n",
       "      <td>CommonApp</td>\n",
       "      <td>Early Action I</td>\n",
       "      <td>...</td>\n",
       "      <td>640.0</td>\n",
       "      <td>1</td>\n",
       "      <td>89.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2.222222</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1290.0</td>\n",
       "      <td>Texas</td>\n",
       "      <td>06</td>\n",
       "      <td>South, West South Central</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15143 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID train-test Entry Term (Application) Permanent Country Sex  \\\n",
       "0          1      train                Fall 2017     United States   F   \n",
       "1          2      train                Fall 2019     United States   M   \n",
       "2          3      train                Fall 2020     United States   F   \n",
       "3          4      train                Fall 2017     United States   F   \n",
       "4          5      train                Fall 2019     United States   M   \n",
       "...      ...        ...                      ...               ...  ..   \n",
       "15138  15139       test                Fall 2020     United States   F   \n",
       "15139  15140       test                Fall 2019     United States   F   \n",
       "15140  15141       test                Fall 2020     United States   M   \n",
       "15141  15142       test                Fall 2018     United States   F   \n",
       "15142  15143       test                Fall 2019     United States   F   \n",
       "\n",
       "                 Ethnicity                       Race        Religion  \\\n",
       "0      Non Hispanic/Latino                      White  Roman Catholic   \n",
       "1      Non Hispanic/Latino                      White   Not specified   \n",
       "2          Hispanic/Latino                      White       Christian   \n",
       "3      Non Hispanic/Latino                      Asian   Not specified   \n",
       "4      Non Hispanic/Latino                      White       Christian   \n",
       "...                    ...                        ...             ...   \n",
       "15138  Non Hispanic/Latino                      Asian   Not specified   \n",
       "15139  Non Hispanic/Latino                      Asian           Hindu   \n",
       "15140  Non Hispanic/Latino  Black or African American   Not specified   \n",
       "15141  Non Hispanic/Latino                      White   Not specified   \n",
       "15142  Non Hispanic/Latino                      White  Roman Catholic   \n",
       "\n",
       "      Application Source     Decision Plan  ... SAT R Math Section Decision  \\\n",
       "0              CommonApp   Early Action II  ...                NaN        1   \n",
       "1              CommonApp    Early Action I  ...              720.0        0   \n",
       "2             ApplyTexas      Early Action  ...                NaN        1   \n",
       "3              CommonApp    Early Action I  ...                NaN        0   \n",
       "4              CommonApp  Regular Decision  ...              700.0        0   \n",
       "...                  ...               ...  ...                ...      ...   \n",
       "15138          CommonApp  Regular Decision  ...                NaN        0   \n",
       "15139          CommonApp    Early Action I  ...                NaN        0   \n",
       "15140          CommonApp      Early Action  ...                NaN        0   \n",
       "15141          CommonApp    Early Action I  ...              740.0        0   \n",
       "15142          CommonApp    Early Action I  ...              640.0        1   \n",
       "\n",
       "      Submit_FirstSource Submit_Inquiry School 1 Top Percent in Class  \\\n",
       "0                   -1.0          -13.0                     10.094637   \n",
       "1                   89.0           11.0                     13.878470   \n",
       "2                   38.0           11.0                     29.182240   \n",
       "3                   72.0            2.0                      4.112150   \n",
       "4                   54.0           11.0                     18.601583   \n",
       "...                  ...            ...                           ...   \n",
       "15138               97.0           11.0                     16.224847   \n",
       "15139               36.0            8.0                      9.422111   \n",
       "15140               18.0           11.0                     29.182240   \n",
       "15141               88.0            4.0                      1.785714   \n",
       "15142               89.0           34.0                      2.222222   \n",
       "\n",
       "      School 2 Top Percent in Class   SAT R       State code  \\\n",
       "0                               NaN    None  New Mexico   01   \n",
       "1                               NaN  1380.0       Texas   22   \n",
       "2                               NaN    None       Texas   15   \n",
       "3                               NaN    None  Washington   05   \n",
       "4                               NaN  1350.0       Texas   06   \n",
       "...                             ...     ...         ...  ...   \n",
       "15138                           NaN    None  California   16   \n",
       "15139                           NaN    None       Texas   16   \n",
       "15140                           NaN    None   Minnesota   01   \n",
       "15141                           NaN  1510.0       Texas   23   \n",
       "15142                           NaN  1290.0       Texas   06   \n",
       "\n",
       "                            Region  \n",
       "0                   West, Mountain  \n",
       "1        South, West South Central  \n",
       "2        South, West South Central  \n",
       "3                    West, Pacific  \n",
       "4        South, West South Central  \n",
       "...                            ...  \n",
       "15138                West, Pacific  \n",
       "15139    South, West South Central  \n",
       "15140  Midwest, West North Central  \n",
       "15141    South, West South Central  \n",
       "15142    South, West South Central  \n",
       "\n",
       "[15143 rows x 47 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TU = TU.join(Regions, on = 'State', how = 'left')\n",
    "TU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6baaf894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Empty DataFrame\n",
      "Columns: [ID, train-test, Entry Term (Application), Permanent Country, Sex, Ethnicity, Race, Religion, Application Source, Decision Plan, Legacy, Athlete, Sport 1 Sport, Sport 1 Rating, Sport 2 Sport, Sport 3 Sport, Academic Interest 1, Academic Interest 2, First_Source Origin First Source Summary, Total Event Participation, Count of Campus Visits, School 1 GPA Recalculated, ACT Composite, SAT R Evidence-Based Reading and Writing Section + Math Section, Permanent Geomarket, Citizenship Status, Academic Index, Intend to Apply for Financial Aid?, Merit Award, Test Optional, SAT I Critical Reading, SAT I Math, SAT I Writing, SAT R Evidence-Based Reading and Writing Section, SAT R Math Section, Decision, Submit_FirstSource, Submit_Inquiry, School 1 Top Percent in Class, School 2 Top Percent in Class, SAT R, State, Region]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 43 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "South, West South Central                                10258\n",
       "West, Pacific                                             1322\n",
       "International                                              913\n",
       "West, Mountain                                             781\n",
       "South, South Atlantic                                      532\n",
       "Midwest, West North Central                                361\n",
       "South, East South Central                                  296\n",
       "Midwest, East North Central                                295\n",
       "Northeast, Middle Atlantic                                 212\n",
       "Northeast, New England                                     149\n",
       "Other U.S. Region (other than states and territories)       22\n",
       "U.S. Territories (not classified into census regions)        2\n",
       "Name: Region, dtype: int64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i,b in TU.iterrows():\n",
    "   # Handles territories and International cases\n",
    "    if (TU['State'][i] == \"PR\") & (pd.isna(TU['Region'][i])):\n",
    "           TU['Region'][i] = \"U.S. Territories (not classified into census regions)\"\n",
    "    elif (TU['State'][i] == \"INT\") & (pd.isna(TU['Region'][i])):\n",
    "            TU['Region'][i] = \"International\"\n",
    "\n",
    "print(TU['Region'].isna().sum())\n",
    "\n",
    "print(TU[TU['Region'].isna()])\n",
    "\n",
    "TU['Region'] = TU['Region'].fillna(\"Other U.S. Region (other than states and territories)\")  #Handles cases where the data point is in the U.S.\n",
    "TU['Region'].value_counts()\n",
    "# Use Region column in modeling stage because it provides regional location for applicants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "87b99076",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>train-test</th>\n",
       "      <th>Entry Term (Application)</th>\n",
       "      <th>Permanent Country</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Ethnicity</th>\n",
       "      <th>Race</th>\n",
       "      <th>Religion</th>\n",
       "      <th>Application Source</th>\n",
       "      <th>Decision Plan</th>\n",
       "      <th>...</th>\n",
       "      <th>SAT R Evidence-Based Reading and Writing Section</th>\n",
       "      <th>SAT R Math Section</th>\n",
       "      <th>Decision</th>\n",
       "      <th>Submit_FirstSource</th>\n",
       "      <th>Submit_Inquiry</th>\n",
       "      <th>School 1 Top Percent in Class</th>\n",
       "      <th>School 2 Top Percent in Class</th>\n",
       "      <th>SAT R</th>\n",
       "      <th>State</th>\n",
       "      <th>Region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>Fall 2017</td>\n",
       "      <td>United States</td>\n",
       "      <td>F</td>\n",
       "      <td>Non Hispanic/Latino</td>\n",
       "      <td>White</td>\n",
       "      <td>Roman Catholic</td>\n",
       "      <td>CommonApp</td>\n",
       "      <td>Early Action II</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-13.0</td>\n",
       "      <td>10.094637</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>New Mexico</td>\n",
       "      <td>West, Mountain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>train</td>\n",
       "      <td>Fall 2019</td>\n",
       "      <td>United States</td>\n",
       "      <td>M</td>\n",
       "      <td>Non Hispanic/Latino</td>\n",
       "      <td>White</td>\n",
       "      <td>Not specified</td>\n",
       "      <td>CommonApp</td>\n",
       "      <td>Early Action I</td>\n",
       "      <td>...</td>\n",
       "      <td>660.0</td>\n",
       "      <td>720.0</td>\n",
       "      <td>0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>13.878470</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1380.0</td>\n",
       "      <td>Texas</td>\n",
       "      <td>South, West South Central</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>train</td>\n",
       "      <td>Fall 2020</td>\n",
       "      <td>United States</td>\n",
       "      <td>F</td>\n",
       "      <td>Hispanic/Latino</td>\n",
       "      <td>White</td>\n",
       "      <td>Christian</td>\n",
       "      <td>ApplyTexas</td>\n",
       "      <td>Early Action</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>29.182240</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>Texas</td>\n",
       "      <td>South, West South Central</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>train</td>\n",
       "      <td>Fall 2017</td>\n",
       "      <td>United States</td>\n",
       "      <td>F</td>\n",
       "      <td>Non Hispanic/Latino</td>\n",
       "      <td>Asian</td>\n",
       "      <td>Not specified</td>\n",
       "      <td>CommonApp</td>\n",
       "      <td>Early Action I</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.112150</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>Washington</td>\n",
       "      <td>West, Pacific</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>train</td>\n",
       "      <td>Fall 2019</td>\n",
       "      <td>United States</td>\n",
       "      <td>M</td>\n",
       "      <td>Non Hispanic/Latino</td>\n",
       "      <td>White</td>\n",
       "      <td>Christian</td>\n",
       "      <td>CommonApp</td>\n",
       "      <td>Regular Decision</td>\n",
       "      <td>...</td>\n",
       "      <td>650.0</td>\n",
       "      <td>700.0</td>\n",
       "      <td>0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>18.601583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1350.0</td>\n",
       "      <td>Texas</td>\n",
       "      <td>South, West South Central</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15138</th>\n",
       "      <td>15139</td>\n",
       "      <td>test</td>\n",
       "      <td>Fall 2020</td>\n",
       "      <td>United States</td>\n",
       "      <td>F</td>\n",
       "      <td>Non Hispanic/Latino</td>\n",
       "      <td>Asian</td>\n",
       "      <td>Not specified</td>\n",
       "      <td>CommonApp</td>\n",
       "      <td>Regular Decision</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>16.224847</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>California</td>\n",
       "      <td>West, Pacific</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15139</th>\n",
       "      <td>15140</td>\n",
       "      <td>test</td>\n",
       "      <td>Fall 2019</td>\n",
       "      <td>United States</td>\n",
       "      <td>F</td>\n",
       "      <td>Non Hispanic/Latino</td>\n",
       "      <td>Asian</td>\n",
       "      <td>Hindu</td>\n",
       "      <td>CommonApp</td>\n",
       "      <td>Early Action I</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.422111</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>Texas</td>\n",
       "      <td>South, West South Central</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15140</th>\n",
       "      <td>15141</td>\n",
       "      <td>test</td>\n",
       "      <td>Fall 2020</td>\n",
       "      <td>United States</td>\n",
       "      <td>M</td>\n",
       "      <td>Non Hispanic/Latino</td>\n",
       "      <td>Black or African American</td>\n",
       "      <td>Not specified</td>\n",
       "      <td>CommonApp</td>\n",
       "      <td>Early Action</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>29.182240</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>Minnesota</td>\n",
       "      <td>Midwest, West North Central</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15141</th>\n",
       "      <td>15142</td>\n",
       "      <td>test</td>\n",
       "      <td>Fall 2018</td>\n",
       "      <td>United States</td>\n",
       "      <td>F</td>\n",
       "      <td>Non Hispanic/Latino</td>\n",
       "      <td>White</td>\n",
       "      <td>Not specified</td>\n",
       "      <td>CommonApp</td>\n",
       "      <td>Early Action I</td>\n",
       "      <td>...</td>\n",
       "      <td>770.0</td>\n",
       "      <td>740.0</td>\n",
       "      <td>0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.785714</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1510.0</td>\n",
       "      <td>Texas</td>\n",
       "      <td>South, West South Central</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15142</th>\n",
       "      <td>15143</td>\n",
       "      <td>test</td>\n",
       "      <td>Fall 2019</td>\n",
       "      <td>United States</td>\n",
       "      <td>F</td>\n",
       "      <td>Non Hispanic/Latino</td>\n",
       "      <td>White</td>\n",
       "      <td>Roman Catholic</td>\n",
       "      <td>CommonApp</td>\n",
       "      <td>Early Action I</td>\n",
       "      <td>...</td>\n",
       "      <td>650.0</td>\n",
       "      <td>640.0</td>\n",
       "      <td>1</td>\n",
       "      <td>89.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2.222222</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1290.0</td>\n",
       "      <td>Texas</td>\n",
       "      <td>South, West South Central</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15143 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID train-test Entry Term (Application) Permanent Country Sex  \\\n",
       "0          1      train                Fall 2017     United States   F   \n",
       "1          2      train                Fall 2019     United States   M   \n",
       "2          3      train                Fall 2020     United States   F   \n",
       "3          4      train                Fall 2017     United States   F   \n",
       "4          5      train                Fall 2019     United States   M   \n",
       "...      ...        ...                      ...               ...  ..   \n",
       "15138  15139       test                Fall 2020     United States   F   \n",
       "15139  15140       test                Fall 2019     United States   F   \n",
       "15140  15141       test                Fall 2020     United States   M   \n",
       "15141  15142       test                Fall 2018     United States   F   \n",
       "15142  15143       test                Fall 2019     United States   F   \n",
       "\n",
       "                 Ethnicity                       Race        Religion  \\\n",
       "0      Non Hispanic/Latino                      White  Roman Catholic   \n",
       "1      Non Hispanic/Latino                      White   Not specified   \n",
       "2          Hispanic/Latino                      White       Christian   \n",
       "3      Non Hispanic/Latino                      Asian   Not specified   \n",
       "4      Non Hispanic/Latino                      White       Christian   \n",
       "...                    ...                        ...             ...   \n",
       "15138  Non Hispanic/Latino                      Asian   Not specified   \n",
       "15139  Non Hispanic/Latino                      Asian           Hindu   \n",
       "15140  Non Hispanic/Latino  Black or African American   Not specified   \n",
       "15141  Non Hispanic/Latino                      White   Not specified   \n",
       "15142  Non Hispanic/Latino                      White  Roman Catholic   \n",
       "\n",
       "      Application Source     Decision Plan  ...  \\\n",
       "0              CommonApp   Early Action II  ...   \n",
       "1              CommonApp    Early Action I  ...   \n",
       "2             ApplyTexas      Early Action  ...   \n",
       "3              CommonApp    Early Action I  ...   \n",
       "4              CommonApp  Regular Decision  ...   \n",
       "...                  ...               ...  ...   \n",
       "15138          CommonApp  Regular Decision  ...   \n",
       "15139          CommonApp    Early Action I  ...   \n",
       "15140          CommonApp      Early Action  ...   \n",
       "15141          CommonApp    Early Action I  ...   \n",
       "15142          CommonApp    Early Action I  ...   \n",
       "\n",
       "      SAT R Evidence-Based Reading and Writing Section SAT R Math Section  \\\n",
       "0                                                  NaN                NaN   \n",
       "1                                                660.0              720.0   \n",
       "2                                                  NaN                NaN   \n",
       "3                                                  NaN                NaN   \n",
       "4                                                650.0              700.0   \n",
       "...                                                ...                ...   \n",
       "15138                                              NaN                NaN   \n",
       "15139                                              NaN                NaN   \n",
       "15140                                              NaN                NaN   \n",
       "15141                                            770.0              740.0   \n",
       "15142                                            650.0              640.0   \n",
       "\n",
       "      Decision Submit_FirstSource Submit_Inquiry  \\\n",
       "0            1               -1.0          -13.0   \n",
       "1            0               89.0           11.0   \n",
       "2            1               38.0           11.0   \n",
       "3            0               72.0            2.0   \n",
       "4            0               54.0           11.0   \n",
       "...        ...                ...            ...   \n",
       "15138        0               97.0           11.0   \n",
       "15139        0               36.0            8.0   \n",
       "15140        0               18.0           11.0   \n",
       "15141        0               88.0            4.0   \n",
       "15142        1               89.0           34.0   \n",
       "\n",
       "      School 1 Top Percent in Class School 2 Top Percent in Class   SAT R  \\\n",
       "0                         10.094637                           NaN    None   \n",
       "1                         13.878470                           NaN  1380.0   \n",
       "2                         29.182240                           NaN    None   \n",
       "3                          4.112150                           NaN    None   \n",
       "4                         18.601583                           NaN  1350.0   \n",
       "...                             ...                           ...     ...   \n",
       "15138                     16.224847                           NaN    None   \n",
       "15139                      9.422111                           NaN    None   \n",
       "15140                     29.182240                           NaN    None   \n",
       "15141                      1.785714                           NaN  1510.0   \n",
       "15142                      2.222222                           NaN  1290.0   \n",
       "\n",
       "            State                       Region  \n",
       "0      New Mexico               West, Mountain  \n",
       "1           Texas    South, West South Central  \n",
       "2           Texas    South, West South Central  \n",
       "3      Washington                West, Pacific  \n",
       "4           Texas    South, West South Central  \n",
       "...           ...                          ...  \n",
       "15138  California                West, Pacific  \n",
       "15139       Texas    South, West South Central  \n",
       "15140   Minnesota  Midwest, West North Central  \n",
       "15141       Texas    South, West South Central  \n",
       "15142       Texas    South, West South Central  \n",
       "\n",
       "[15143 rows x 46 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TU.drop('code', axis = 'columns', inplace = True)\n",
    "TU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b3fc8e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Citizenship Status and Academic Index - no data cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f6576a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0    10222\n",
       "0.0     4921\n",
       "Name: Intend to Apply for Financial Aid?, dtype: int64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Financial Aid Data Cleanup \n",
    "# Find missing values\n",
    "print(TU['Intend to Apply for Financial Aid?'].isna().sum())\n",
    "# 21 Nas\n",
    "\n",
    "# Fill missing variables with \"0\"(since it's either 1 or 0) and since they didn't indicate a 1 for applying financial aid, assume 0 (they don't have an intent to apply for financial aid)\n",
    "TU['Intend to Apply for Financial Aid?'].fillna(0, inplace = True)\n",
    "TU['Intend to Apply for Financial Aid?'].value_counts().astype('int') #This is to double check the handling the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "87517cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Non-Full Ride    14835\n",
       "Full Ride          170\n",
       "No Merit           138\n",
       "Name: Merit Award, dtype: int64"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merit Award Data Cleanup\n",
    "print(TU['Merit Award'].isna().sum())\n",
    "# 0 NAS\n",
    " \n",
    "\n",
    "Merit_Awards = pd.read_excel('Admissions Data_Merit Award Codes.xlsx')\n",
    "Merit_Awards     \n",
    "\n",
    "\n",
    "# Recategorize all levels into groups - referring to the Merit_Awards Excel file, I use awards ending in 0 and assigning them as no merit, \n",
    "# Any row with 'SEM' or 'TTS ' as \"Full Ride\" and others are assigned as non-full ride.\n",
    "\n",
    "\n",
    "def merit_award_amount(merit_award):\n",
    "    if merit_award in ['X0', 'Y0', 'TTS', 'SEM']:    # since these awards are noted as full ride, I have these reclassified as full ride\n",
    "        return \"Full Ride\"\n",
    "    elif merit_award in ['Z0', 'I0']:    # Since these awards have 0, they are no merit\n",
    "        return \"No Merit\"\n",
    "    else:   # Since all the other awards are less than $50,0000 but more than 0, they are non-full ride merit awards\n",
    "        return \"Non-Full Ride\"\n",
    "\n",
    "\n",
    "TU['Merit Award'] = TU['Merit Award'].apply(merit_award_amount)\n",
    "\n",
    "TU['Merit Award'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b1f25620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(TU['Merit Award'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fceb2103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6711\n"
     ]
    }
   ],
   "source": [
    "# SAT Concordance Score (of SAT R) Data Cleanup\n",
    "print(TU['SAT Concordance Score (of SAT R)'].isna().sum())\n",
    "# 6711 NAS\n",
    "# Since the SAT test changed in 2018, this column is rendered obsolete. Remove this column\n",
    "TU.drop(\"SAT Concordance Score (of SAT R)\", axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c570da37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8222\n"
     ]
    }
   ],
   "source": [
    "# ACT Concordance Score (of SAT R)\n",
    "print(TU['ACT Concordance Score (of SAT R)'].isna().sum())\n",
    "# 8222 NAs\n",
    "# Since the SAT scores were changed, remove the variable\n",
    "TU.drop(\"ACT Concordance Score (of SAT R)\", axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "27fb1d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15120\n"
     ]
    }
   ],
   "source": [
    "# ACT Concordance Score (of SAT)\n",
    "print(TU['ACT Concordance Score (of SAT)'].isna().sum())\n",
    "# 15120 NAs\n",
    "# Since the SAT scores were changed, remove the variable\n",
    "TU.drop(\"ACT Concordance Score (of SAT)\", axis='columns', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4f30b873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision - no cleanup since this is the dependent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae72ff5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26b3335",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
