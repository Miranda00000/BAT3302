{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a16bb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Wrangling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8a5927f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a8bbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Creation\n",
    "from prophet import Prophet\n",
    "from xgboost import XGBRegressor\n",
    "from category_encoders import OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedb50a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data \n",
    "sunoco = pd.read_csv('Sunoco Project Data  - Combined Variables Monthly Data.csv', index_col = 'Month')\n",
    "\n",
    "# Removing data that will be replaced when importing weekly data\n",
    "columns_to_remove = ['Automobile Sales (In millions)']\n",
    "for cname in columns_to_remove:\n",
    "    sunoco = sunoco.drop(cname, axis=1)\n",
    "\n",
    "# COLUMN Categorizations\n",
    "# All of the known columns from the data set\n",
    "all_column_names = list(sunoco.columns)\n",
    "# Columns which can be interpolated linearly to get weekly data from monthly\n",
    "linear_columns = ['U.S. Demographics (in millions)']\n",
    "# Columns which we have raw weekly data for and so we don't need to interpolate at all\n",
    "weekly_data_columns = [\n",
    "    'Regular Gasoline Prices (dollars per gallon)',\n",
    "    'Crude oil prices (U.S.) (Dollars per barrel)',\n",
    "    'Federal Funds Effective Rate',\n",
    "    'Implied Gasoline Demand',\n",
    "    'US Gross inputs to refineries (in thousands)',\n",
    "    'US Percent Utilization of Refinery Operable Capacity',\n",
    "    'Automobile Sales (In millions)',\n",
    "    'Total Ridership (in thousands)',\n",
    "    'Weekly Jobless Claims'\n",
    "]\n",
    "# Columns which we don't have weekly data for or cannot interpolate (e.g. because we don't have the data or they are index fields)\n",
    "other_columns = list((set(all_column_names) - set(weekly_data_columns)) - set(linear_columns))\n",
    "\n",
    "\n",
    "\n",
    "sunoco.index = pd.to_datetime(sunoco.index,utc='True')\n",
    "sunoco['Date'] = sunoco.index\n",
    "# print(sunoco.columns)\n",
    "# sunoco = sunoco.loc[sunoco.Date >= '2022-01-01']\n",
    "sunoco = sunoco.tz_localize(None)\n",
    "\n",
    "sunoco_no_interpolation = sunoco.copy()\n",
    "# print(sunoco_no_interpolation.head())\n",
    "sunoco_no_interpolation.index = pd.to_datetime(sunoco_no_interpolation.index,utc='True')\n",
    "sunoco_no_interpolation['Date'] = sunoco_no_interpolation.index\n",
    "sunoco_no_interpolation.drop(columns=linear_columns, axis=1, inplace=True)\n",
    "sunoco_no_interpolation = sunoco_no_interpolation.resample('D').ffill().resample('W').ffill()\n",
    "# sunoco_no_interpolation.index = pd.to_datetime(sunoco_no_interpolation.index,utc='True')\n",
    "# sunoco_no_interpolation['Date'] = sunoco_no_interpolation.index\n",
    "# sunoco_no_interpolation = pd.DataFrame(sunoco_no_interpolation.resample('W'))\n",
    "\n",
    "\n",
    "sunoco_no_interpolation[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0191a9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sunoco_weekly_data = pd.read_csv('Sunoco Project Data  - Combined Variables Weekly Data.csv', index_col = 'Week', )[weekly_data_columns]\n",
    "\n",
    "sunoco_weekly_data.head()\n",
    "sunoco_weekly_data.index = pd.to_datetime(sunoco_weekly_data.index,utc='True')\n",
    "sunoco_weekly_data['Week'] = sunoco_weekly_data.index\n",
    "sunoco_weekly_data = sunoco_weekly_data.resample('D').ffill().resample('W').ffill()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35c96b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sunoco_weekly_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae59831b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change to weekly - upsampling\n",
    "sunoco_linear = sunoco[linear_columns]\n",
    "sunoco_linear=sunoco_linear.resample('D')\n",
    "sunoco_linear = sunoco_linear.interpolate(method='linear', )\n",
    "\n",
    "\n",
    "sunoco_linear = sunoco_linear.resample('W').interpolate(method='linear', )\n",
    "sunoco_linear.index = pd.to_datetime(sunoco_linear.index,utc='True')\n",
    "sunoco_linear['Date'] = sunoco_linear.index\n",
    "sunoco_linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc3c590",
   "metadata": {},
   "outputs": [],
   "source": [
    "sunoco_joined = sunoco_linear.join(sunoco_no_interpolation[other_columns], on='Date', how='inner', lsuffix = 'Month', rsuffix = 'Date')\n",
    "sunoco_joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a650ba8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO figure out why 57 rows are not being joined when using a 'left' join\n",
    "sunoco_weekly_joined = sunoco_joined.join(sunoco_weekly_data, on = 'Date', how = 'inner', lsuffix = 'Week', rsuffix = 'Date')\n",
    "sunoco_weekly_joined.set_index('Week', inplace=True)\n",
    "sunoco_weekly_joined.drop(columns=['Date'], axis=1, inplace=True)\n",
    "sunoco_weekly_joined.sort_values('Week', inplace=True)\n",
    "sunoco_weekly_joined = sunoco_weekly_joined.reset_index('Week')\n",
    "sunoco_weekly_joined.Week = pd.to_datetime(sunoco_weekly_joined.Week, unit='D')\n",
    "sunoco_weekly_joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e7b8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sunoco_weekly_joined['Week'] = sunoco_weekly_joined['Week'].dt.tz_localize(None)\n",
    "sunoco_weekly_joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecf056c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in sunoco_weekly_joined.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663c9398",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_percent_str(val):\n",
    "    try:\n",
    "        return val.str.rstrip(\"%\").astype(float)/100\n",
    "    except AttributeError:\n",
    "        return val\n",
    "\n",
    "def convert_dollar_str(val):\n",
    "    try:\n",
    "        return val.str.lstrip(\" $\").astype(float)\n",
    "    except AttributeError:\n",
    "        return val\n",
    "# and then add the following below the \"other_columns\" assignment in cell 5:\n",
    "\n",
    "percentage_columns = [\n",
    "    'IE (25th Percentile)',\n",
    "    'IE (75th Percentile)',\n",
    "    'IE (Median)',\n",
    "    'HME (25th Percentile)',\n",
    "    'HME (75th percentile)',\n",
    "    'HME (Median)',\n",
    "    'Labor costs (Govt) (in percents)',\n",
    "    'Labor costs (Private) (in percents)',\n",
    "]\n",
    "\n",
    "dollar_columns = [\n",
    "    'Employee Earnings'\n",
    "]\n",
    "sunoco_weekly_joined[percentage_columns] = sunoco_weekly_joined[percentage_columns].apply(convert_percent_str)\n",
    "sunoco_weekly_joined[dollar_columns] = sunoco_weekly_joined[dollar_columns].apply(convert_dollar_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe17412c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning - handle missing values in weekly joined dataset \n",
    "missing_backward_vals_cols = ['Employee Earnings', 'HME (25th Percentile)','HME (Median)', 'HME (75th percentile)', 'IE (25th Percentile)', 'IE (Median)', 'IE (75th Percentile)', 'EV Regulations (total)', 'BEV Sales (U.S.)', 'US Dollar Index', 'Consumer Credit Data', 'Consumer loans (Dollars)', 'Labor costs (Private) (in percents)', 'Labor costs (Govt) (in percents)']\n",
    "missing_forward_vals_cols = ['BEV Sales (U.S.)', 'Labor costs (Private) (in percents)', 'Labor costs (Govt) (in percents)', 'Employee Earnings', 'U.S. Crude Oil and Natural Gas Rotary Rigs in Operation (Count)', 'US GDP (in billions)', 'Consumer Credit Data', 'EV Regulations (total)']\n",
    "# Use bfill to fill missing values - what I found on the internet\n",
    "for col in missing_backward_vals_cols:\n",
    "    sunoco_weekly_joined[col] = sunoco_weekly_joined[col].bfill()\n",
    "# Use ffill to fill missing values left\n",
    "for col in missing_forward_vals_cols:\n",
    "    sunoco_weekly_joined[col] = sunoco_weekly_joined[col].ffill()\n",
    "sunoco_weekly_joined['Week'] = pd.to_datetime(sunoco_weekly_joined['Week'])\n",
    "sunoco_weekly_joined = sunoco_weekly_joined.set_index('Week')\n",
    "sunoco_weekly_joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26886dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Further data cleaning:\n",
    "#US Gross inputs to refineries (in thousands)\n",
    "print(f\"Nas: {sunoco_weekly_joined['US Gross inputs to refineries (in thousands)'].isna().sum()}\")\n",
    "mean_3 = sunoco_weekly_joined['US Gross inputs to refineries (in thousands)'].mean()\n",
    "sunoco_weekly_joined['US Gross inputs to refineries (in thousands)'] = sunoco_weekly_joined['US Gross inputs to refineries (in thousands)'].fillna(mean_3)\n",
    "print(f\"Nas after imputing mean: {sunoco_weekly_joined['US Gross inputs to refineries (in thousands)'].isna().sum()}\")\n",
    "\n",
    "#US Percent Utilization of Refinery Operable Capacity\n",
    "print(sunoco_weekly_joined['US Percent Utilization of Refinery Operable Capacity'].dtype)\n",
    "#Deal with NAs\n",
    "print(sunoco_weekly_joined['US Percent Utilization of Refinery Operable Capacity'].isna().sum())\n",
    "mean_4 = sunoco_weekly_joined['US Percent Utilization of Refinery Operable Capacity'].mean()\n",
    "sunoco_weekly_joined['US Percent Utilization of Refinery Operable Capacity'] = sunoco_weekly_joined['US Percent Utilization of Refinery Operable Capacity'].fillna(mean_4)\n",
    "print(sunoco_weekly_joined['US Percent Utilization of Refinery Operable Capacity'].isna().sum())\n",
    "\n",
    "\n",
    "#Automobile Sales (In millions)\n",
    "print(sunoco_weekly_joined['Automobile Sales (In millions)'].dtype)\n",
    "#Check nas \n",
    "print(sunoco_weekly_joined['Automobile Sales (In millions)'].isna().sum())\n",
    "#No nas \n",
    "\n",
    "#Total Ridership (in thousands)\n",
    "print(sunoco_weekly_joined['Total Ridership (in thousands)'].dtype)\n",
    "#Check nas \n",
    "print(sunoco_weekly_joined['Total Ridership (in thousands)'].isna().sum())\n",
    "#No nas \n",
    "#Nas from April 23 to Sept 23 - will impute using mean of 2023\n",
    "ridership_2023 = sunoco_weekly_joined[sunoco_weekly_joined.index.year == 2023]\n",
    "mean_2023 = ridership_2023['Total Ridership (in thousands)'].mean() \n",
    "sunoco_weekly_joined['Total Ridership (in thousands)'] = sunoco_weekly_joined['Total Ridership (in thousands)'].fillna(mean_2023)\n",
    "print(sunoco_weekly_joined['Total Ridership (in thousands)'].isna().sum())\n",
    "\n",
    "#Weekly Jobless Claims\n",
    "print(sunoco_weekly_joined['Weekly Jobless Claims'].dtype)\n",
    "#Check nas \n",
    "print(sunoco_weekly_joined['Weekly Jobless Claims'].isna().sum())\n",
    "#No nas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060c00be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7 day moving average \n",
    "sunoco_weekly_joined['Moving Average'] = sunoco_weekly_joined['Implied Gasoline Demand'].rolling(7).mean()\n",
    "sunoco_weekly_joined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f4d3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use prophet to predict implied gasoline demand\n",
    "def index_to_column(data):\n",
    "    data = data.reset_index()\n",
    "    data['Week'] = pd.to_datetime(data['Week'])\n",
    "    data = data.sort_values('Week')\n",
    "  \n",
    "    \n",
    "    data = data.rename(columns={'Week': 'ds', 'Implied Gasoline Demand': 'y'})\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75edefb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sunoco_weekly_joined_train, sunoco_weekly_joined_test = sunoco_weekly_joined[sunoco_weekly_joined.index < '2021-01-01'], sunoco_weekly_joined[sunoco_weekly_joined.index >='2021-01-01']\n",
    "print('Train:\\t', len(sunoco_weekly_joined_train))\n",
    "print('Test:\\t', len(sunoco_weekly_joined_test)) \n",
    "\n",
    "prophet_train = index_to_column(sunoco_weekly_joined_train)\n",
    "prophet_test = index_to_column(sunoco_weekly_joined_test)\n",
    "prophet_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afe2f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prophet_model = Prophet(interval_width=0.95)\n",
    "\n",
    "prophet_model.fit(prophet_train)\n",
    "prophet_pred = prophet_model.predict(prophet_test[['ds']])\n",
    "prophet_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6a167d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = round(mean_absolute_error(prophet_test['y'], prophet_pred['yhat']), 3)\n",
    "print(mae)\n",
    "plt.figure(figsize=(20,8), dpi=100)\n",
    "plt.plot(prophet_test['ds'], prophet_test['y'], label='Actual')\n",
    "plt.plot(prophet_pred['ds'], prophet_pred['yhat'], label='Predicted')\n",
    "plt.title('Test Forecasting', weight='bold', fontsize=40)\n",
    "plt.title('Testing Set Forecast', weight='bold', fontsize=25)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4465df",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "plt.plot(sunoco_weekly_joined_train.index, sunoco_weekly_joined_train['Implied Gasoline Demand'], label=\"Training set\")\n",
    "plt.plot(sunoco_weekly_joined_test.index, sunoco_weekly_joined_test['Implied Gasoline Demand'], label=\"Test set\")\n",
    "plt.axvline(pd.to_datetime('2021-01-01'), color='black', ls='--', lw=2)\n",
    "plt.text(pd.to_datetime('2021-01-01'), y=40, s='Split', fontsize=10, fontweight='bold')\n",
    "plt.title('Data Splitting', weight='bold', fontsize=20)\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46abc562",
   "metadata": {},
   "source": [
    "7-Day and 30-day forecasts with Prophet - use training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a11fdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This time, we will use all data (train and test) to train our model\n",
    "new_sunoco_weekly_joined = index_to_column(sunoco_weekly_joined)\n",
    "new_sunoco_weekly_joined\n",
    "# Data Cleaning: Handle missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8443a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "prophet_model2 = Prophet(interval_width=0.95, seasonality_mode=\"multiplicative\")\n",
    "prophet_model2.fit(new_sunoco_weekly_joined)\n",
    "\n",
    "future_dates = prophet_model2.make_future_dataframe(periods=7, freq='W')\n",
    "prophet_pred2 = prophet_model2.predict(future_dates)\n",
    "\n",
    "plt.figure(figsize=(20,8))\n",
    "\n",
    "fig = prophet_model2.plot(prophet_pred2, uncertainty=True)\n",
    "ax = fig.gca()\n",
    "ax.set_xlim(pd.to_datetime(['2021-01-01', '2023-09-07']))\n",
    "plt.title('7 Days Forecast', weight='bold', fontsize=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ccc740",
   "metadata": {},
   "outputs": [],
   "source": [
    "prophet_model.plot_components(prophet_pred2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9787f782",
   "metadata": {},
   "outputs": [],
   "source": [
    "prophet_pred2 = prophet_pred2.set_index('ds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ae3f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sunoco_weekly_joined_with_yhat = sunoco_weekly_joined.join(prophet_pred2['yhat'], how = 'inner')\n",
    "sunoco_weekly_joined_with_yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b60600",
   "metadata": {},
   "outputs": [],
   "source": [
    "future_dates2 = prophet_model2.make_future_dataframe(periods=30, freq='W')\n",
    "prophet_pred3 = prophet_model2.predict(future_dates2)\n",
    "\n",
    "plt.figure(figsize=(20,8))\n",
    "\n",
    "fig = prophet_model2.plot(prophet_pred3, uncertainty=True)\n",
    "ax = fig.gca()\n",
    "ax.set_xlim(pd.to_datetime(['2021-01-01', '2023-09-15']))\n",
    "plt.title('30 Days Forecast', weight='bold', fontsize=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ffa385",
   "metadata": {},
   "source": [
    "XGBoost and Random Forest Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcc6c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sunoco_weekly_joined_with_yhat.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5eea62",
   "metadata": {},
   "outputs": [],
   "source": [
    "sunoco_weekly_joined_with_yhat.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b74392",
   "metadata": {},
   "outputs": [],
   "source": [
    "sunoco_weekly_joined_with_yhat.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30390f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sunoco_weekly_joined_with_yhat.index.name = 'Week'\n",
    "sunoco_weekly_joined_with_yhat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa5155d",
   "metadata": {},
   "outputs": [],
   "source": [
    "round(((sunoco_weekly_joined_with_yhat.isnull().sum()/len(sunoco_weekly_joined_with_yhat))*100),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260d9bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sunoco_weekly_joined.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556568e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "heat_map_features = sunoco_weekly_joined\n",
    "\n",
    "# Set Figure Size\n",
    "plt.figure(figsize=(15,40))\n",
    "\n",
    "# .corr heatmap of df to visualize correlation & show plot\n",
    "sns.heatmap(round(heat_map_features.corr(),1),annot=True,cmap='Blues',linewidth=0.9)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0af411e",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = sunoco_weekly_joined.corr(method='pearson')\n",
    "print(correlations['Implied Gasoline Demand'].sort_values(ascending=False).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea05a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Target variable\n",
    "target='Implied Gasoline Demand'\n",
    "\n",
    "# Split data into feature matrix and target vector\n",
    "y,X=sunoco_weekly_joined[target],sunoco_weekly_joined.drop(columns=target)\n",
    "\n",
    "# split data into train / validation sets\n",
    "X_train,X_val,y_train,y_val = train_test_split(X,y,test_size=.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54390b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f33cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [y_train.mean()]*len(y_train)\n",
    "mean_baseline_pred = y_train.mean()\n",
    "baseline_mae = mean_absolute_error(y_train,y_pred)\n",
    "baseline_rmse = mean_squared_error(y_train,y_pred,squared=False)\n",
    "\n",
    "# Print statement to show all baseline values\n",
    "print('Mean Price Per KW/h Baseline Pred:', mean_baseline_pred)\n",
    "print('-------------------------------------------------------------------')\n",
    "print('Baseline Mae:',baseline_mae)\n",
    "print('-------------------------------------------------------------------')\n",
    "print('Baseline RMSE:',baseline_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedbc7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal = OrdinalEncoder()\n",
    "ordinal_fit = ordinal.fit(X_train)\n",
    "XT_train = ordinal.transform(X_train)\n",
    "XT_val = ordinal.transform(X_val)\n",
    "XT_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5263e04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "simp = SimpleImputer(strategy='mean')\n",
    "simp_fit = simp.fit(XT_train)\n",
    "XT_train = simp.transform(XT_train)\n",
    "XT_val = simp.transform(XT_val)\n",
    "XT_val[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b66ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning model variables\n",
    "model_rfr = RandomForestRegressor()\n",
    "model_xgbr=XGBRegressor()\n",
    "\n",
    "# Fitting models\n",
    "model_rfr.fit(XT_train,y_train);\n",
    "model_xgbr.fit(XT_train,y_train);\n",
    "\n",
    "# Def to check model metrics of baseline performance\n",
    "def check_metrics(model):\n",
    "    print(model)\n",
    "    print('===================================================================')\n",
    "    print('Training MAE:', mean_absolute_error(y_train,model.predict(XT_train)))\n",
    "    print('-------------------------------------------------------------------')\n",
    "    print('Validation MAE:', mean_absolute_error(y_val,model.predict(XT_val)))\n",
    "    print('-------------------------------------------------------------------')\n",
    "    print('Validation R2 score:', model.score(XT_val,y_val))\n",
    "    print('===================================================================')\n",
    "model = [model_xgbr,model_rfr]\n",
    "for m in model:\n",
    "  check_metrics(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4736e27",
   "metadata": {},
   "source": [
    "Random Forest Regressor seems to perform a little better than XGBRegressor in this model without the y-hat.  The Validation R2 score of the Random Forest Regressor is approximately 0.862, whereas the Validation R2 score of the XGBoost Regressor is approximately 0.854. The validation mean absolute error for the Random Forest Regressor is smaller compared to with XGBoost without y-hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62eb4073",
   "metadata": {},
   "source": [
    "Time Series Analysis with y-hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b6689b",
   "metadata": {},
   "outputs": [],
   "source": [
    "heat_map_features = sunoco_weekly_joined_with_yhat\n",
    "\n",
    "# Set Figure Size\n",
    "plt.figure(figsize=(15,12.5))\n",
    "\n",
    "# .corr heatmap of df to visualize correlation & show plot\n",
    "sns.heatmap(round(heat_map_features.corr(),1),annot=True,cmap='Blues',linewidth=0.9)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5375bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = sunoco_weekly_joined_with_yhat.corr(method='pearson')\n",
    "print(correlations['Implied Gasoline Demand'].sort_values(ascending=False).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8633a745",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create Target variable\n",
    "target='Implied Gasoline Demand'\n",
    "\n",
    "# Split data into feature matrix and target vector\n",
    "y,X=sunoco_weekly_joined_with_yhat[target],sunoco_weekly_joined_with_yhat.drop(columns=target)\n",
    "\n",
    "# split data into train / validation sets\n",
    "X_train,X_val,y_train,y_val = train_test_split(X,y,test_size=.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185014c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949cf841",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [y_train.mean()]*len(y_train)\n",
    "mean_baseline_pred = y_train.mean()\n",
    "baseline_mae = mean_absolute_error(y_train,y_pred)\n",
    "baseline_rmse = mean_squared_error(y_train,y_pred,squared=False)\n",
    "\n",
    "# Print statement to show all baseline values\n",
    "print('Mean Price Per KW/h Baseline Pred:', mean_baseline_pred)\n",
    "print('-------------------------------------------------------------------')\n",
    "print('Baseline Mae:',baseline_mae)\n",
    "print('-------------------------------------------------------------------')\n",
    "print('Baseline RMSE:',baseline_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587369bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal = OrdinalEncoder()\n",
    "ordinal_fit = ordinal.fit(X_train)\n",
    "XT_train = ordinal.transform(X_train)\n",
    "XT_val = ordinal.transform(X_val)\n",
    "XT_train\n",
    "XT_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac36315c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot Encoder to transform Seasons column\n",
    "onehot = OneHotEncoder()\n",
    "onehot_fit = onehot.fit(X_train)\n",
    "XT_train = onehot.transform(X_train)\n",
    "XT_val = onehot.transform(X_val)\n",
    "XT_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e27e9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple imputer to fill nan values, then transform sets\n",
    "simp = SimpleImputer(strategy='mean')\n",
    "simp_fit = simp.fit(XT_train)\n",
    "XT_train = simp.transform(XT_train)\n",
    "XT_val = simp.transform(XT_val)\n",
    "XT_val[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25caed13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning model variables\n",
    "model_rfr = RandomForestRegressor()\n",
    "model_xgbr=XGBRegressor()\n",
    "\n",
    "# Fitting models\n",
    "model_rfr.fit(XT_train,y_train);\n",
    "model_xgbr.fit(XT_train,y_train);\n",
    "\n",
    "# Def to check model metrics of baseline performance\n",
    "def check_metrics(model):\n",
    "    print(model)\n",
    "    print('===================================================================')\n",
    "    print('Training MAE:', mean_absolute_error(y_train,model.predict(XT_train)))\n",
    "    print('-------------------------------------------------------------------')\n",
    "    print('Validation MAE:', mean_absolute_error(y_val,model.predict(XT_val)))\n",
    "    print('-------------------------------------------------------------------')\n",
    "    print('Validation R2 score:', model.score(XT_val,y_val))\n",
    "    print('===================================================================')\n",
    "model = [model_xgbr,model_rfr]\n",
    "for m in model:\n",
    "  check_metrics(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c74292",
   "metadata": {},
   "source": [
    "With the y-hat variable, Random Forest Regressor performs better than XGBoost Regressor with the Validation R2 score with the Random Forest Regressor being 0.87768, compared to that of XGBoost (0.8713). However, the difference in performance level between XGBoost and Random Forest Regressor is smaller with y-hat compared to without the y-hat. The Validation mean absolute error in the Random Forest Regressor is 187.7 (which is a smaller MAE compared to Random Forest Regressor without the y-hat variable). Additionally, while the XGBoost Validation Mean Absolute Error is greater than that of Random Forest even with the y-hat, the value is reduced to 194.8 with the y-hat from 211.7 without the y-hat. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fa4918",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autosave 15\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cc8b74",
   "metadata": {},
   "source": [
    "Both XGBoost and Random Forest Regressor Time Series Analysis perform better with the y-hat variable compared to without the y-hat variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab01e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: 3 Tuning in the Random Forest Regressor and XGB Regressor (data cleaning - use wrangle function)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128396b9",
   "metadata": {},
   "source": [
    "#4 Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63795063",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "result = permutation_importance(model_xgbr, XT_val, y_val,\n",
    "                          n_repeats=30,\n",
    "                         random_state=0)\n",
    "imdf=pd.DataFrame(result.importances_mean).T\n",
    "importances = imdf.set_axis(onehot_fit.get_feature_names_out(), axis=1)\n",
    "importances.T[0].sort_values(ascending=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f02184",
   "metadata": {},
   "source": [
    "#5. Conclusions/major findings regarding IV's importance to impacting DV:\n",
    "1. yhat is important when predicting Implied Gasoline Demand\n",
    "2. Crude oil prices, Employee Earnings, and Regular Gasoline appear to be the most significant predictors of Implied Gasoline Demand (as those predictors have most prominent feature importance)\n",
    "3. Consumer loans, Housing Market Expectations, Federal Funds Effective Rate, Personal Consumption Expenditures, US GDP, US Dollar Index, Inflationary Expectations, Rig Count, U.S. Demographics, and the total EV Regulations show moderate feature importance, meaning that they have a moderate impact on gasoline demand.\n",
    "4. US Spot Market, Labor Costs, Refinery Util, and Automobile Sales appear to have little to no impact on predicting gasoline demand as their feature importance is small. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7896cd2c",
   "metadata": {},
   "source": [
    "#6. Are our hypotheses supported or rejected?\n",
    "1. Our hypothesis concerning the effect of U.S. Demographics on Implied Gasoline Demand is rejected as there is a positive (not a negative) correlation between U.S. population and Implied Gasoline Demand. Additonally, our hypothetical magnitude is rejected as the correlation coefficient is moderate/medium correlation value (as opposed to less than 0.3).\n",
    "2. Our Hypothesis regarding EV Sales is wrong as our hypothesis suggests that EV Sales have a high impact on Implied Gasoline demand, while the correlation coefficient produced suggests that this variable has no magnitude/impact on the DV (even with a negative value). \n",
    "3. EV Regulations does appear to have a meaningful magnitude on Gasoline Demand (supporting the magnitude side of the hypothesis). However, it does not have a negative correlation with Implied Gasoline Demand as the correlation coefficient is positive (meaning that the initial hypothesis is partially rejected).\n",
    "4. Our Hypothesis concerning Regular Gasoline Prices appears to be partially supported (the magnitude is correct, with the correlation coefficent indicating a medium correlation between Regular Gasoline Prices and Implied Gasoline Prices. However, the hypothetical direction is incorrect as the correlation coefficient suggests a positive impact on Implied Gasoline Demand (not a negative one). \n",
    "5. Our hypothesis concerning Crude Oil Prices is partially supported (our magnitude is supported, but the hypothetical direction is rejected). Crude Oil Prices have a moderate effect on Implied Gasoline Demand (given that the correlation coefficient is greater than 0.3). However, given that the correlation coefficient is positive, the hypothetical direction is rejected. \n",
    "6. Our hypothesis regarding U.S. GDP appears to be partially correct. The correlation coefficent between US GDP and Implied Gasoline Demand rejects the magnitude as the correlation coefficient (0.495) suggests that US GDP has a medium magnitude impact on Implied Gasoline Demand. However, the direction in which U.S. GDP impacts the DV appears to be supported as the correlation coefficient is positive (which adds up to our hypothesis).\n",
    "7. Hypothesis regarding Rig Count appears to be supported by our findings. The correlation coefficient between Rig Count and Implied Gasoline Demand supports that this predictor has a moderate impact and a positive correlation on Gasoline Demand.\n",
    "8. Hypothesis relating to Federal Funds Effective Rate and how it impacts gasoline demand is partially correct as it does have a medium significance on predicting Implied Gasoline Demand, but the direction of the relationship is  positive, not negative as previously predicted.\n",
    "9. Hypotheses relating to the impact and direction of Housing Market Expectations and Inflationary Expectations on Implied Gasoline Demand appears to be correct as both have a low impact on gasoline demand and are inversely related to the DV.\n",
    "10. Our hypothesis regarding consumer credit data and US Spot market and their impact on Implied Gasoline demand is rejected as the Feature Importance does indicate that these variables have low (if any) impact on predictions of Gasoline Demand. \n",
    "11. Our hypothesis about total ridership was rejected, as total ridership is positively correlated with Gasoline Demand, and the magnitude is low, as it has a low importance value. \n",
    "12. Our hypothesis about refinery utilization were partially correct. This was a relatively important feature, but its magnitude was not high. However, both variables - US gross inputs to refineries and US percent utilization of refineries are positively correlated with Gasoline Demand.\n",
    "13. Our hypothesis about jobless claims was partially correct, as it is negatively correlated. However, the magnitude of thhis variable is not high, as it has a low importance value. \n",
    "14. Our hypothesis about automobile sales was partially correct, as it is positively correlated to Implied Gasoline Demand. However, the magnitude of this variable is not high, as it is not a strong correlation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcaffe6",
   "metadata": {},
   "source": [
    "#7. Implications of my conclusions\n",
    "1. As U.S. population increases in the near future, we can predict that future Gasoline Demand will increase alongside it (especially with more Americans being at least 16 years of age (driving age)). \n",
    "2. The more EV Regulations that are passed in the future will result in Gasoline Demand increasing. Though, EV Sales will have a very low impact on increasing gasoline demand. \n",
    "3. As employee earnings are constantly increasing, we are likely to see an increase in gasoline demand as there is more revenue for the consumer to spend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71a28d38",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sunoco_weekly_joined_with_yhat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sunoco_weekly_joined_with_yhat\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msunoco_weekly_joined_with_yhat\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sunoco_weekly_joined_with_yhat' is not defined"
     ]
    }
   ],
   "source": [
    "sunoco_weekly_joined_with_yhat.to_csv('sunoco_weekly_joined_with_yhat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf6ba53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
